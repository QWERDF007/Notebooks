# [GLEE: General Object Foundation Model for Images and Videos at Scale](https://arxiv.org/abs/2312.09158)

## Abstract

在这篇论文中，我们提出了 GLEE，这是一个用于定位和识别图像和视频中物体的对象级基础模型。通过一个统一的框架，GLEE 在开放世界的场景下完成了任意物体的检测、分割、跟踪、定位和识别，适用于各种物体感知任务。GLEE 采用一种内聚的学习策略，从具有不同监督级别的各种数据源中获取知识，以形成通用对象表示，在零样本迁移到新数据和任务方面表现出色。具体来说，我们使用图像编码器、文本编码器和视觉提示器来处理多模态输入，从而能够同时解决各种以对象为中心的下游任务，同时保持最先进的性能。通过对来自不同基准测试的超过五百万张图像进行大量训练，GLEE 展示了卓越的多功能性和改进的泛化性能，可以高效地处理下游任务，而无需针对特定任务进行调整。通过集成大量自动标记的数据，我们进一步增强了它的零样本泛化能力。此外，GLEE 可以集成到大型语言模型中，作为基础模型为多模态任务提供通用对象级信息。我们希望该方法的多功能性和通用性将标志着高效视觉基础模型在 AGI 系统开发方面迈出重要一步。该模型和代码将在 https://glee-vision.github.io/ 发布。

## 1. Introduction

基础模型 [7] 是构建人工通用智能 (AGI) 系统的一个新兴范式，基础模型指的是一种经过广泛数据训练，能够适应各种下游任务的模型。近年来，NLP 的基础模型，例如 BERT [22]、GPT-3 [9] 和 T5 [78] 等采用统一的输入-输出范式和大规模预训练，已经取得了令人瞩目的泛化能力，可以应对几乎所有 NLP 任务。

在计算机视觉领域，任务类型的多样性和缺乏统一的框架使得视觉基础模型只能服务于特定的子领域，例如 CLIP 用于多模态视觉模型，MAE 用于视觉表示模型，SAM 用于分割模型。尽管这些模型已经广泛研究，但当前的视觉基础模型仍专注于建立全局图像特征与语言描述之间的关联，或者学习图像级特征表示。然而，定位和识别对象构成了计算机视觉系统的基本能力，是解决分割、场景理解、物体跟踪、事件检测和活动识别等复杂或高级视觉任务的基础，并支持广泛的应用。

在这项工作中，我们推进了视觉领域中的对象级基础模型的发展。为了解决上述局限性，提供通用且准确的对象级信息，我们引入了一种通用的对象视觉基础模型，命名为 GLEE，该模型同时解决了一系列以对象为中心的任务，并确保了最先进技术的性能，包括对象检测、实例分割、定位、对象跟踪、交互式分割和跟踪等，如图 1 所示。通过统一的输入和输出范式定义，我们的模型能够从各种多样的数据中学习并预测通用的对象表示，使其能够以零样本的方式很好地推广到新的数据和任务，并取得惊人的性能。此外，由于统一的范式，通过引入大量自动标记的数据，可以低成本地扩展训练数据，并进一步提高模型的零样本泛化能力。

**通用对象基础模型框架**。我们的目标是构建一个能够同时解决广泛以对象为中心任务的对象视觉基础模型。具体而言，我们使用图像编码器、文本编码器和视觉提示器来对多模态输入进行编码。它们被集成到一个检测器中，根据文本和视觉输入从图像中提取对象。这种统一的处理多模态方法使我们能够同时解决各种以对象为中心的任务，包括检测、实例分割、指代表达理解、交互式分割、多目标跟踪、视频实例分割、视频实例分割和视频参考分割，同时保持最先进的性能。

**一个多粒度联合监督和可扩展训练范式**。统一的框架能够解决多项任务，其设计使能够对来自不同基准测试和具有不同监督级别的超过 500 万张图像进行联合训练。现有的数据集在注释粒度上有所不同：Objects365 [88] 和 OpenImages [46] 等检测数据集提供边界框和类别名称；COCO [58] 和 LVIS [32] 提供更细粒度的掩码注释；RefCOCO [72, 120] 和 Visual Genome [44] 提供详细的物体描述。此外，视频数据增强了模型的时序一致性，而开放世界数据则提供了与类别无关的物体注释。图 2 直观地展示了所用数据集的监督类型和数据规模。我们方法对多源数据的统一支持大大方便了额外的手动或自动注释数据的加入，从而能够轻松扩展数据集。此外，跨任务模型优化的对齐意味着联合训练不仅作为一个统一策略，还作为一个提升各个单独任务性能的机制。

**强大的零样本可迁移性以适用于广泛的物体级图像和视频任务**。经过来自不同来源的数据的联合训练后，GLEE 展示了卓越的多功能性和零样本泛化能力。大量实验表明，GLEE 在对象级图像任务（例如检测、指称表达理解和开放世界检测）方面达到了与现有专家模型和通用模型相比的最先进的性能，而这一切都不需要任何特定任务的设计或微调。此外，我们展示了 GLEE 在大词汇量开放世界视频跟踪任务中的卓越泛化和零样本能力，即使在零样本迁移方式下，也取得了比现有模型明显更优的性能。此外，通过结合像 SA1B [43] 和 GRIT [75] 这样的自动注释的数据，我们可以将我们的训练数据集以低成本扩展到令人印象深刻的 1000 万张图像，这对于对象级任务通常很难实现，并进一步增强了泛化性能。此外，我们将多模态大型语言模型 (mLLM) [47] 中的 SAM [43] 组件替换为 GLEE，并观察到它取得了可比较的结果。这表明 GLEE 能够提供当前大型语言模型所缺少的视觉对象级信息，从而为以对象为中心的 mLLM 奠定坚实基础。



## 5. Conclusion

我们提出了一种名为 GLEE 的前沿对象级基础模型，旨在直接适用于各种对象级图像和视频任务。GLEE 采用统一的学习范式进行设计，可以从具有不同监督级别的各种数据源中学习。GLEE 在众多对象级任务上实现了最先进的性能，并且在零样本迁移到新数据和任务方面表现出色，展示了其卓越的多功能性和泛化能力。此外，GLEE 提供了当前大型语言模型所缺少的通用视觉对象级信息，为以对象为中心的多模态大型语言模型 (mLLMs) 奠定了坚实的基础。   

