# [A COMPREHENSIVE REVIEW OF YOLO: FROM YOLOV1 AND BEYOND](https://arxiv.org/abs/2304.00501)

## Abstract

YOLO 已经成为机器人、无人驾驶汽车和视频监控应用的核心实时目标检测系统。我们全面分析了 YOLO 的演变，检验了从原始 YOLO 到 YOLOv8、YOLO-NAS 和带 Transformer 的 YOLO 每一代的创新和贡献。我们首先描述标准指标和后处理，然后讨论每个模型在网络架构和训练技巧上的主要变化。最后，我们总结 YOLO 开发的重要经验教训，并对其未来提供视角，重点介绍增强实时目标检测系统性能的潜在研究方向。

**Keywords** YOLO · Object detection · Deep Learning · Computer Vision

## 1 Introduction

实时目标检测已成为许多应用中的关键组成部分，涵盖了自动驾驶汽车、机器人、视频监控和增强现实等各个领域。在不同的目标检测算法中，YOLO (You Only Look Once) 框架因其出色的速度和准确性而脱颖而出，使图像中的物体能够快速且可靠地被识别出来。自诞生以来，YOLO 系列已经经历了多次迭代，每一次都在前一版本的基础上进行改进，以解决局限性并提高性能 (见图 1)。本文旨在全面回顾 YOLO 框架的发展，从最初的 YOLOv1 到最新的 YOLOv8，阐明了每个版本之间的关键创新、差异和改进。

除了 YOLO 框架之外，目标检测和图像处理领域还发展出了其他一些值得注意的方法。例如，R-CNN (基于区域的卷积神经网络) [1]及其后续版本 Fast R-CNN [2]和 Faster R-CNN [3]在提高目标检测的准确性方面发挥了关键作用。这些方法依赖于一个两阶段的过程：选择性搜索生成区域候选，卷积神经网络对这些区域进行分类和细化。另一个重要的方法是 Single-Shot MultiBox Detector (SSD)，类似于 YOLO，它专注于速度和效率，通过消除独立的区域建议步骤来实现。此外，像Mask R-NN 这样的方法扩展了实例分割的能力，实现了精确的目标定位和像素级分割。这些发展，以及其他方法如 RetinaNet 和 EfficientDet，共同构成了目标检测算法多样化的格局。每种方法都在速度、准确性和复杂性之间存在独特的权衡，以满足不同的应用需求和计算限制。

其他伟大的综述包括[8,9,10]。 但是 [8] 的综述只包括到 YOLOv3，[9] 只包括到YOLOv4，落后于最新发展。我们的论文与[10]不同，展示了大多数 YOLO 架构的深入架构，并涵盖其他变体，如 YOLOX、PP-YOLO、带 transformers 的 YOLO 和 YOLO-NAS。

本文首先探讨了原始 YOLO 模型的基础概念和架构，这为 YOLO 系列的后续进步奠定了基础。在此之后，我们深入探讨了从 YOLOv2 到 YOLOv8 的每个版本中引入的改进和增强。这些改进涵盖了网络设计、损失函数修改、锚框适应以及输入分辨率缩放等各个方面。通过检查这些发展，我们旨在全面理解 YOLO 框架的演变及其对目标检测的意义。

除了讨论每个 YOLO 版本的具体改进，本文还突出了 YOLO 框架发展过程中速度与准确性之间的权衡。这强调了在选择最合适的 YOLO 模型时，考虑特定应用的上下文和要求的重要性。最后，我们展望了 YOLO 框架的未来发展方向，概述了进一步研究和开发的潜在途径，这将塑造实时目标检测系统的持续进步。

<img src="./assets/yolo_review_fig1.jpg">

图 1：YOLO 版本的时间线

## 2 YOLO Applications Across Diverse Fields

YOLO 的实时目标检测能力对自主驾驶汽车系统具有莫大价值，可以快速识别和跟踪各种对象，如汽车、行人、自行车和其他障碍物。这些能力已经被应用于许多领域，包括视频监控中的行为识别、体育分析和人机交互。

YOLO 模型已被用于农业领域检测和分类农作物、病虫害，协助精准农业技术和自动化农业流程。它们还被改编用于生物识别、安全和面部识别系统中的人脸检测任务。

在医学领域，YOLO 已被用于癌症检测、皮肤分割和药片识别，提高了诊断准确率和治疗过程的效率。在遥感领域，它被用于卫星和航空图像中的目标检测和分类，协助土地利用制图、城市规划和环境监测。

安全系统集成了 YOLO 模型进行实时视频监控和分析，允许快速检测可疑活动、社交距离和戴口罩检测。这些模型还被应用于表面检测以检测缺陷和异常情况，提高了制造业和生产过程中的质量控制。

在交通应用中，YOLO 模型被用于车牌检测和交通标志识别等任务，有助于智能交通系统和交通管理解决方案的发展。它们被用于野生动物检测和监测，以识别濒危物种用于生物多样性保护和生态系统管理。最后，YOLO 被广泛用于机器人应用和无人机目标检测。图 2 显示了 Scopus 中找到的所有论文的文献计量网络可视化标题中包含 YOLO 一词，并通过对象检测关键字进行过滤。然后，我们手动过滤所有的论文与应用程序相关。

<img src="./assets/yolo_review_fig2.jpg">

**图 2**：使用 [?] 创建的主要 YOLO 应用程序的文献计量网络可视化

## 3 Object Detection Metrics and Non-Maximum Suppression (NMS)

平均准确率 (Average Precision，AP)，传统上称为 Mean Average Precision，mAP，是评估目标检测模型性能的常用指标。它测量所有类别的平均准确率，提供一个值来比较不同模型。COCO 数据集不区分 AP 和 mAP。在本文其余部分，我们将此指标称为 AP。

在 YOLOv1 和 YOLOv2 中，用于训练和基准测试的数据集是 PASCAL VOC 2007 和 VOC 2012。然而，从 YOLOv3 开始，所用数据集是 Microsoft COCO (上下文中的常见物体)。这些数据集的 AP 计算方式不同。下节将讨论 AP 背后的原理，并解释其计算方式。

### 3.1 How AP works?

AP 指标基于精确率-召回率指标，处理多个物体类别，并使用交并比 (Intersection over Union， IoU) 来定义正样本预测。

**精确率和召回率**：精确率衡量模型正样本预测的准确度，而召回率衡量模型正确识别的实际正样本的比例。精确率和召回率通常存在权衡；例如，增加检测物体数量 (更高的召回率) 可能导致更多假正样本 (更低的精确率)。为了考虑这种权衡，AP 指标结合了随不同置信度阈值绘制精确率-召回率曲线。该指标通过考虑精确率-召回率曲线下的面积来平衡评估精确率和召回率。

**处理多个物体类别**：目标检测模型必须识别图像中的多个物体类别并定位。AP 指标通过分别计算每个类别的平均准确率 (AP)，然后计算所有类别的 AP 均值来解决这个问题 (这也是为什么它也称为 mAP)。这种方法确保每个类别的性能都被单独评估，为模型的整体性能提供了更全面的评估。

**交并比**：目标检测旨在通过预测边界框准确定位图像中的物体。AP 指标结合使用交并比 (IoU) 测量来评估预测边界框的质量。IoU 是预测边界框和真值边界框交集面积与并集面积之比 (见图 2)。它测量真值和预测边界框之间的重叠面积。COCO 基准考虑多个 IoU 阈值来评估模型在不同定位准确度水平上的性能。

<img src="./assets/yolo_review_fig3.jpg">

**图 3**：交并比 (IoU)。a) IoU的计算是将两个框的交集除以框的并集；b) 三个不同IoU值的示例，对应不同的框位置。

### 3.2 Computing AP

AP 在 VOC 和 COCO 数据集上的计算方式不同。本节将描述它在每个数据集上的计算方式。

#### VOC 数据集

该数据集包含 20 个物体类别。在 VOC 中计算 AP 的步骤如下:

1. 对每个类别，通过改变模型预测的置信度阈值计算精确率-召回率曲线。
2. 使用曲线的 11 点插值计算每个类别的平均准确率 (AP)。
3. 通过计算所有 20 个类别的 APs 均值获得最终的平均准确率 (AP)。

#### Microsoft COCO 数据集

该数据集包含 80 个物体类别，使用更复杂的方法计算 AP。它使用 101 点插值而不是 11 点插值，即它以 0.01 的步长计算从 0 到 1 的 101 个召回率阈值的精确率。此外，**AP 是通过多个 IoU 值的平均获得的**，而不仅仅是一个，除了一个常用的 AP 指标 $AP_{50}$ ，它是 IoU 阈值为 0.5 时的 AP。

在 COCO 中计算 AP 的步骤如下:

1. 对每个类别，通过改变模型预测的置信度阈值计算精确率-召回率曲线。
2. 使用 101 个召回率阈值计算每个类别的平均准确率 (AP)。
3. 计算不同交并比 (IoU) 阈值的 AP，通常从 0.5 到 0.95，步长为 0.05。更高的 IoU 阈值需要更准确的预测才能被视为真正例。
4. 对每个 IoU 阈值，计算所有 80 个类别的 AP 均值。
5. 最后，通过对每个 IoU 阈值计算的 AP 值平均获得总体 AP。

AP 计算的差异使得很难直接比较两个数据集上目标检测模型的性能。当前的标准是使用 COCO AP，因为它可以更细致地评估模型在不同 IoU 阈值下的表现。

### 3.3 Non-Maximum Suppression (NMS)

非极大值抑制 (Non-Maximum Suppression, NMS) 是目标检测算法中使用的一种后处理技术，用于减少重叠的边界框数量并提高整体检测质量。目标检测算法通常为同一目标生成多个具有不同置信度分数的边界框。NMS过滤掉冗余和不相关的边界框，只保留最准确的那些。算法 1 描述了该过程。图 4 显示了目标检测模型的典型输出，包含多个重叠的边界框，以及 NMS 后的输出。

<img src="./assets/yolo_review_fig4.jpg">

**图 4**：非极大值抑制 (NMS)。a) 显示了目标检测模型的典型输出，其中包含多个重叠的框。b) 显示 NMS 后的输出。

<img src="./assets/yolo_review_nms_algo.jpg">


> NMS 的工作原理如下:
>
> 
> 0. 过滤低于置信度阈值的边界框。
> 1. 对检测结果按置信度分数排序。
> 2. 选择置信度最高的边界框。
> 3. 计算其他所有边界框与该边界框的交集比例(IoU)。
> 4. 删除IoU大于阈值(如0.5)的边界框。
> 5. 重复步骤2-4，直到没有边界框剩余。
>
> 这样就可以删除大部分冗余和不准确的边界框，只保留置信度高且不重叠的边界框。NMS 是目标检测后处理中的标准操作，极大地提升了检测性能。

## 4 YOLO: You Only Look Once

YOLO 由 Joseph Redmon 等人在 2016 年 CVPR 上发表。它首次提出了一个端到端的实时目标检测方法。YOLO 代表“You Only Look Once”，意思是它能够在网络的一次前向传播中完成检测任务，而之前的方法要么使用滑动窗口后跟一个需要对每个图像运行数百次或数千次的分类器，要么使用更先进的两阶段方法，其第一步检测可能存在目标的区域或候选区域，第二步对候选区域运行分类器。此外，YOLO 使用了一种更直接的仅基于回归的输出来预测检测结果，而不是 Fast R-CNN 使用的两种独立输出，一种用于概率的分类，一种用于框坐标的回归。

### 4.1 How YOLOv1 works?

YOLOv1 通过同时检测所有边界框来统一目标检测步骤。为实现此目的，YOLO 将输入图像划分为一个 $S \times S$ 的网格，并为每个网格单元预测 $B$ 个同类边界框以及 $C$ 个不同类别的置信度。每个边界框预测由五个值组成： $P_c, bx, by, bh, bw$ ，其中 $P_c$ 是边界框置信度，反映模型对该框包含物体的置信程度及框的准确性。 $bx$ 和 $by$ 是框相对于网格单元的中心坐标， $bh$ 和 $bw$ 是框相对于整个图像的高度和宽度。YOLO 的输出是一个 $S \times S \times (B \times 5 + C)$ 的张量，可选地后面跟着非极大值抑制 (NMS) 来消除重复检测。

在原始 YOLO 论文中，作者使用 PASCAL VOC 数据集，其中包含 20 类 $(C=20)$ ; $7\times7$ 网格 $(S=7)$ ；每个网格单元最多 2 类 $(B=2)$ ，给出 $7 \times 7 \times 30$ 的输出预测。

图 5 显示了考虑 $3 \times 3$ 网格、3 类和每个网格 1 类的简化输出向量，共 8 个值。在这个简化案例中，YOLO 的输出就是 $3 \times 3 \times 8$ 。

YOLOv1 在 PASCAL VOC2007 数据集上达到了 63.4 的平均准确率 (AP)。

<img src="./assets/yolo_review_fig5.jpg">

**图 5**：YOLO 输出预测。该图描绘了一个简化的 YOLO 模型， $3 \times 3$ 的网格，3 个类别，以及每个网格元素的单类预测以产生 8 个值的向量。

### 4.2 YOLOv1 Architecture

YOLOv1 的架构由 24 个卷积层和两个全连接层组成，全连接层预测边界框坐标和概率。除了最后一层使用线性激活函数外，所有层都使用了泄露修正线性单元 (ReLU) 激活函数。受 GoogLeNet 和 Network in Network 的启发，YOLO 使用 $1 \times 1$ 卷积层来减少特征图数量并使参数量相对较低。表 1 描述了 YOLOv1 的架构。作者还引入了一个较轻的模型 Fast YOLO，由 9 个卷积层组成。

<img src="./assets/yolo_review_table1.jpg">

**表 1**：YOLOv1 的架构表格。该架构包括 24 个卷积层，将 $3 \times 3$ 卷积与 $1 \times 1$ 卷积相结合以进行通道缩减。输出是一个全连接层，生成一个 $7 \times 7$ 的网格，每个网格单元格有 30 个值，以容纳 10 个边界框坐标 (2个框) 和 20 个类别。

### 4.3 YOLOv1 Training

作者使用 ImageNet 数据集以 224×224 分辨率预训练了 YOLO 的前 20 层。然后，他们添加了带有随机初始化权重的最后四层，并使用 PASCAL VOC 2007 和VOC 2012 数据集以 448×448 分辨率微调模型，以增加细节来进行更准确的目标检测。

对于数据增强方面，作者使用了最大为输入图像大小 20% 的随机缩放和平移，以及 HSV 颜色空间中上限因子为 1.5 的随机曝光和饱和度。

YOLOv1 使用了由多个平方和误差组成的损失函数，如图 6 所示。在损失函数中， $\lambda_{coord} = 5$ 是一个放大比例因子，给予边界框预测更大的重要性， $\lambda_{noobj} = 0.5$ 是一个缩小比例因子，减少不包含目标的框的重要性。

损失函数的前两个项表示定位损失；它计算预测边界框位置 $(x,y)$ 和大小 $(w,h)$ 的误差。注意，这些误差仅在包含对象的框中计算(由 $\mathbb 1_{ij}^{obj}$ 表示)，仅当网格单元中存在对象时才进行惩罚。第三和第四项损失表示置信度损失；第三项测量在框中检测到对象时 $(\mathbb 1_{ij}^{obj})$ 的置信度误差，第四项测量在框中未检测到对象时的置信度误差 $(\mathbb 1_{ij}^{noobj})$ 。由于大多数框是空的，该损失由 $\lambda_{noobj}$ 项加权缩减。最后一个损失分量是分类损失，仅当对象出现在单元中时 $(\mathbb 1_{ij}^{obj})$ ，它测量每个类别的类别条件概率的平方误差。

<img src="./assets/yolo_review_fig6.jpg">

**图 6**：YOLO 的损失函数，包括边界框坐标的定位损失、对象存在或不存在的置信度损失以及类别预测准确性的分类损失。

### 4.4 YOLOv1 Strengths and Limitations

YOLO 的简单架构以及其新颖的全图像一次回归使其比现有目标检测器快得多，允许实时性能。

然而，尽管 YOLO 比任何目标检测器都快，但与 Fast R-CNN 等最先进方法相比，其定位误差较大。这个限制有三个主要原因：

1. 每个网格单元最多只能检测到两个同类对象，限制了预测相近对象的能力。
2. 它难以预测训练数据中未见宽高比的对象。
3. 由于下采样层，它从粗糙的对象特征中学习。

## 5 YOLOv2: Better, Faster, and Stronger

YOLOv2 由 Joseph Redmon 和 Ali Farhadi 在 CVPR 2017 发表。它在原始 YOLO 的基础上进行了多项改进，使其更好、保持相同的速度同时也更强大——能够检测 9000 个类别！这些改进如下：

1. **批量归一化 (BN)**。在所有卷积层上使用批量归一化，提高收敛性并作为正则化器减少过拟合。
2. **高分辨率分类器**。像 YOLOv1 一样，他们使用 ImageNet 在 224×224 上预训练模型。然而，这一次，他们在 ImageNet 上使用 448×448 的分辨率对模型进行了 10 个 epochs 的微调，提高了网络在更高分辨率输入上的性能。
3. **完全卷积**。他们去掉了密集层 (dense layers)，并使用了一个完全卷积的架构。
4. **使用锚框预测边界框**。他们使用一组先验框或锚框，这些是具有预定义形状的框，用于匹配对象的典型形状，如图 7 所示。为每个网格单元定义了多个锚框，并且系统为每个锚框预测坐标和类别。网络输出的大小与每个网格单元中锚框的数量成比例。
5. **尺寸聚类**。选择好的先验框有助于网络学习预测更准确的边界框。作者在训练边界框上运行 k-means 聚类，以找到好的先验框。他们选择了五个先验框，提供了在召回率和模型复杂度之间的良好平衡。
6. **直接位置预测**。与其他预测偏移量的方法不同 ，YOLOv2 遵循相同的哲学，并且预测相对于网格单元的位置坐标。网络为每个单元格预测五个边界框，每个边界框都有五个值 $t_x, t_y, t_w, t_h$ 和 $t_o$ ，其中 $t_o$ 等价于 YOLOv1 中的 $P_c$ ，最终边界框坐标如图 7 所示。
7. **更细粒度的特征**。与 YOLOv1 相比，YOLOv2 去掉了一个池化层，以获得 416×416 输入图像的 13×13 的输出特征映射或网格。YOLOv2 还使用了一个直通 (passthrough) 层，它将 26×26×512 特征映射重新组织，将相邻的特征堆叠到不同的通道中，而不是通过空间子采样丢失它们。这生成了 13×13×2048 特征映射，这些特征映射在通道维度上与较低分辨率的 13×13×1024 映射拼接在一起，以获得 13×13×3072 特征映射。有关架构详细信息，请参见表 2。
8. **多尺度训练**。由于 YOLOv2 不使用全连接层，因此输入可以具有不同的大小。为了使 YOLOv2 对不同的输入大小具有鲁棒性，作者随机训练模型，每十个批次从 320×320 到 608×608 中更改一次输入大小。

所有这些改进使得 YOLOv2 在 PASCAL VOC2007 数据集上的平均精度 (AP) 达到了 78.6％，而 YOLOv1 仅为 63.4％。

<img src="./assets/yolo_review_fig7.jpg">

**图 7**：锚框。YOLOv2 为每个网格单元定义了多个锚框。

<img src="./assets/yolo_review_fig8.jpg">

**图 8**：YOLOv2 的边界框预测图。边界框的中心坐标是通过将预测 $t_x,t_y$ 值应用一个 sigmoid 函数，并通过网格单元格 的位置 $c_x、c_y$ 进行偏移获得的。最终框的宽度和高度使用先验的宽度 $p_w$ 和高度 $p_h$ ，分别由 $e^{t_w}$ 和 $e^{t_h}$ 缩放，其中 $t_w$ 和 $t_h$ 由 YOLOv2 预测得到。

<img src="./assets/yolo_review_table2.jpg">

**表 2**：YOLOv2 的架构。它由 Darknet-19 骨干网络 (第 1 到 23 层) 和检测头组成，检测头由最后四个卷积层和直通 (passthrough) 层组成。穿越层将 26×26×512 的第 17 个输出的特征重新组织为 13×13×2048，然后与第 25 层连接。最终卷积生成一个 13×13 的网格，具有 125 个通道，以容纳 5 个边界框的 25 个预测 (5 个坐标 + 20 个类别)。

### 5.1 YOLOv2 Architecture

YOLOv2 使用的骨干架构称为 *Darknet-19*，包含 19 个卷积层和 5 个最大池化层。与 YOLOv1 的架构类似，它受到了 Network in Network 的启发，在 3×3 的卷积之间使用 1×1 的卷积以减少参数数量。此外，正如上面提到的，他们使用批量归一化来规范化并帮助收敛。

表 2 显示了整个 Darknet-19 骨干架构及其目标检测头。当使用 PASCAL VOC 数据集时，YOLOv2 预测 5 个边界框，每个边界框都有 5 个值和 20 个类别。

目标分类头将最后四个卷积层替换为一个具有 1000 个滤波器的单卷积层，后跟全局平均池化层和 Softmax。

### 5.2 YOLO9000 is a stronger YOLOv2

作者在论文中介绍了一种联合分类和检测的训练方法。它使用了 COCO 中的检测标记数据来学习边界框坐标，并使用 ImageNet 中的分类数据来增加它可以检测的类别数量。在训练过程中，他们将两个数据集结合在一起，这样当使用检测训练图像时，它会反向传播检测网络，而当使用分类训练图像时，它会反向传播架构的分类部分。结果是一个能够检测超过 9000 个类别的 YOLO 模型，因此被称为YOLO9000。

## 6 YOLOv3

YOLOv3 是由 Joseph Redmon 和 Ali Farhadi 于 2018 年在 ArXiv上 发表的。它包含了重大的变化和更大的架构，以保持与最先进的技术同步，同时保持实时性能。以下是相对于 YOLOv2 的变化：

1. **边界框预测**。像 YOLOv2 一样，网络为每个边界框预测四个坐标 $t_x,t_y,t_w,t_h$ ；但是这一次 YOLOv3 使用逻辑回归为每个边界框预测一个物体得分 (objectness score)。这个得分对于与真实标注重叠最多的锚框是 1，对于其余锚框是 0。与 Faster R-CNN 不同，YOLOv3 仅为每个真实标注对象分配一个锚框。此外，如果没有锚框分配给一个对象，则只会产生分类损失而没有定位损失或置信度损失。
2. **类别预测**。他们使用二元交叉熵来训练独立的逻辑分类器，而不是使用 softmax 进行分类，并将问题作为多标签分类。这个改变允许将多个标签分配给同一个框，在一些具有重叠标签的复杂数据集上可能会出现。例如，同一个对象可以是一个人和一个男人。
3. **新的骨干网络**。YOLOv3 具有一个更大的特征提取器，由 53 个卷积层和残差连接组成。第 6.1 节详细描述了这个架构。
4. **空间金字塔池化 (SPP)**。虽然论文中没有提到，但作者还向骨干网络添加了一个修改后的 SPP 块，它将多个最大池化输出连接起来，而不进行子采样 (步幅= 1)，每个输出具有不同的内核大小 $k \times k$ ，其中 $k = 1,5,9, 13$ ，从而允许更大的感受野。这个版本被称为 YOLOv3-spp，是表现最好的版本，将 $AP_{50}$ 提高了 2.7％。
5. **多尺度预测**。与 Feature Pyramid Networks 类似，YOLOv3 在三个不同的尺度上预测三个边界框。第 6.2 节详细描述了多尺度预测机制。
6. **边界框先验**。与 YOLOv2 类似，作者还使用 k-means 算法确定锚框的边界框先验。不同之处在于，在 YOLOv2 中，他们每个单元格使用了五个先验框，而在YOLOv3 中，他们为三个不同尺度使用了三个先验框 (总共 9 个)。

### 6.1 YOLOv3 Architecture

YOLOv3 中的骨干网络被称为 Darknet-53。它用带步长的卷积替换了所有的最大池化层，并添加了残差连接。它总共包含 53 个卷积层。图 9 显示了架构细节。

Darknet-53 骨干网络获得的 Top-1 和 Top-5 准确率与 ResNet-152 相当，但速度快了近 2 倍。

<img src="./assets/yolo_review_fig9.jpg">

**图 9**：YOLOv3 Darknet-53 骨干网络。YOLOv3 的架构由 53 个卷积层组成，每个卷积层都有批量归一化和 Leaky ReLU 激活。此外，残差连接将 1×1 卷积的输入与整个网络中 3×3 卷积的输出相连。这里展示的架构仅包括骨干网络，不包括由多尺度预测组成的检测头。

### 6.2 YOLOv3 Multi-Scale Predictions

除了更大的架构之外，YOLOv3 的一个重要特征是多尺度预测，即在多个网格大小上进行预测。这有助于获得更精细的边界框，并显着提高小物体的预测能力，这是以前版本的 YOLO 的主要弱点之一。

图 10 所示的多尺度检测架构的工作方式如下：第一个标记为 **y1** 的输出等效于 YOLOv2 的输出，其中 13×13 网格定义了输出。第二个输出 **y2** 由 Darknet-53 的 (Res×4) 后的输出和 (Res×8) 后的输出拼接而成。特征图的大小不同，即 13×13 和 26×26 ，因此在拼接之前需要进行上采样操作。最后，使用上采样操作，第三个输出 **y3** 将 26×26 特征图与 52×52 特征图拼接起来。

对于有 80 个类别的 COCO 数据集，每个尺度提供一个形状为 $N \times N \times [3 \times (4+1+80)]$ 的输出张量，其中 N×N 是特征图 (或网格单元) 的大小，3 表示每个单元格的边界框数，4+1 包括四个坐标和物体得分。

<img src="./assets/yolo_review_fig10.jpg">

**图 10**：YOLOv3 的多尺度检测架构。Darknet-53 骨干网络的输出分为三个不同的输出，分别标记为 **y1**、**y2** 和 **y3**，每个输出的分辨率都有所提高。最终预测的边界框使用非极大值抑制进行过滤。CBL (Convolution-BatchNorm-Leaky ReLU) 块包括一个带有批量归一化和泄漏整流线性单元的卷积层。Res 块包含一个 CBL，后面跟着两个带有残差连接的 CBL 结构，如图 9 所示。

### 6.3 YOLOv3 Results

当 YOLOv3 发布时，目标检测的基准从 PASCAL VOC 更改为 Microsoft COCO。因此，从这里开始，所有的 YOLO 都在 MS COCO 数据集中进行评估。YOLOv3-spp 在 20 FPS 下实现了 36.2％ 的平均精度 AP 和 60.6％ 的 $AP_{50}$ ，当时达到了最先进的水平，并且速度快了 2 倍。

## 7 Backbone, Neck, and Head

此时，目标检测器的架构开始被描述为三个部分：骨干网络、中间层和头部。图 11 展示了一个骨干网络 (backbone)、瓶颈层 (neck) 和检测头 (head) 的高级示意图。

backbone 负责从输入图像中提取有用的特征。它通常是一个在大规模图像分类任务 (例如 ImageNet) 上进行训练的卷积神经网络 (CNN)。骨干网络在不同尺度上捕获分层特征，在前面的层中提取较低级的特征 (如边缘和纹理)，在更深层中提取更高级的特征 (如物体部分和语义信息)。

neck 是连接 backbone 和 head 的中间组件。它聚合和优化骨干网络提取的特征，通常着重增强不同尺度上的空间和语义信息。neck 可包括额外的卷积层、特征金字塔网络 (FPN) 或其他机制来改进特征的表示。

head 是目标检测器的最后一个组件；它基于 backbone 和 neck 提供的特征进行预测。它通常由一个或多个执行分类、定位以及最近的实例分割和姿态估计的特定任务的子网络组成。head 处理 neck 提供的特征，为每个目标候选生成预测。最后，后处理步骤 (例如非极大值抑制 (NMS)) 过滤掉重叠的预测，仅保留最可信的检测结果。在 YOLO 模型的剩余部分中，我们将使用 backbone、neck 和 head 来描述这些架构。

<img src="./assets/yolo_review_fig11.jpg">

**图 11**：现代目标检测器的架构可以描述为骨干网络、瓶颈层和检测头。骨干网络通常是一个卷积神经网络 (CNN)，用于从图像中提取不同尺度的关键特征。瓶颈层对这些特征进行精炼，增强空间和语义信息。最后，检测头使用这些精炼后的特征进行目标检测预测。

## 8 YOLOv4

YOLO 有两年没有新版本了。直到 2020 年 4月，Alexey Bochkovskiy、Chien-Yao Wang 和 Hong-Yuan Mark Liao 在 ArXiv 上发表了 YOLOv4 的论文。起初，不同的作者发表新的“官方”YOLO版本感觉很奇怪；然而，YOLOv4 保持了相同的 YOLO 理念——实时、开源、single shot 和 darknet 框架——并且提升非常令人满意，社区很快就接受了这个版本作为官方的 YOLOv4。

YOLOv4 通过试验许多归类为  bag-of-freebies 和 bag-of-specials 的改变来找到最佳平衡。Bag-of-freebies 是仅改变训练策略、增加训练成本而不增加推理时间的方法，最常见的是数据增强。另一方面，bag-of-specials 是稍微增加推理成本但显著提高准确性的方法。这些方法的例子包括扩大感受野、特征融合和后处理等。

我们总结 YOLOv4 的主要变化有以下几点：

- **一个具有 Bag-of-Specials (BoS) 集成的增强架构**。作者尝试了 ResNeXt50、EfficientNet-B3 和 Darknet-53 等多个骨干网络架构。表现最佳的架构是使用一个修改后的 Darknet-53 作为骨干网络 (见图 12)，其使用了跨阶段局部连接 (CSPNet)，以及 Mish 激活函数。对于 neck，他们使用了 YOLOv3-spp 中的修改版空间金字塔池化 (SPP)，以及像 YOLOv3 一样的多尺度预测，但是使用了修改版的路径聚合网络 (PANet)，而不是 FPN，还有一个修改版的空间注意力模块(SAM)。最后，对于检测头，他们使用了 YOLOv3 中的锚框。因此，该模型被称为 CSPDarknet53-PANet-SPP。添加到 Darknet-53 的跨阶段局部连接 (CSP) 有助于降低模型的计算量而保持相同的准确度。与 YOLOv3-spp 一样，SPP 块增加了感受野而不影响推理速度。PANet 的修改版拼接特征而不是像原论文中将它们相加。
- **集成 bag-of-freebies (BoF) 用于高级训练方法**。除了常规的增强如随机亮度、对比度、缩放、裁剪、翻转和旋转，作者还实现了马赛克增强，将四幅图像组合成一幅，从而可以检测到超出通常上下文的对象，并减少了批量归一化所需的大的 mini-batch 大小。对于正则化，他们使用了 DropBlock，它像 Dropout 一样工作，但用于卷积神经网络，以及类别标签平滑。对于检测器，他们添加了 CIoU 损失和跨小批量标准化 (CmBN)，从整个批处理中而不是在常规批处理标准化的单个小批量中收集统计信息。 
- **自对抗训练 (SAT)**。为了使模型对扰动更具鲁棒性，在输入图像上执行对抗攻击，制造真实目标不在图像中的欺骗，但保留原始标签以检测正确的对象。
- **使用遗传算法优化超参数**。为了找到用于训练的最佳超参数，他们在前 10% 的训练周期中使用遗传算法，以及余弦退火调度器在训练过程中改变学习率。学习率先缓慢减小，在训练过程中减小一半，最后略微减小。

表 3 列出了骨干网络和检测器的 BoFs 和 BoS 的最终选择。

在 MS COCO 数据集 test-dev 2017 上进行评估，YOLOv4 在一块 NVIDIA V100 上以超过 50 FPS 取得了 43.5% 的 AP 和 65.7% $AP_{50}$ 。

<img src="./assets/yolo_review_fig12.jpg">

**图 12**：用于目标检测的 YOLOv4 的架构。图中的模块为 **CMB**：卷积 + 批量归一化 + Mish 激活，**CBL**：卷积 + 批量归一化 + Leaky Relu，**UP**：上采样，**SPP**：空间金字塔池化，**PANet**：路径聚合网络。图示灵感来自 [79]。

<img src="./assets/yolo_review_table3.jpg">

**表 3**：YOLOv4 的 bag-of-freebies (BoF) 和 bag-of-specials (BoS) 的最终选择。BoF 是一些在不增加推理成本但会增加训练时间的提高性能的方法。BoS 是一些略微增加推理成本但显著提高准确性的方法。

## 9 YOLOv5

骨干网络是经过修改的 CSPDarknet53，它以一个 Stem 开始，这是一个带有大窗口大小的步进卷积层，用于减少内存和计算成本；之后是从输入图像中提取相关特征的卷积层。SPPF (空间金字塔池化快速) 层和后续的卷积层在各种尺度上处理特征，而上采样层则增加了特征图的分辨率。SPPF 层旨在通过将不同尺度的特征汇聚到固定大小的特征图中，加速网络的计算。每个卷积后面都跟着批量归一化 (BN) 和 SiLU 激活。neck 使用了 SPPF 和修改过的 CSP-PAN，而 head 类似于 YOLOv3。

YOLOv5 使用了多种数据增强技术，例如 Mosaic、复制粘贴、随机仿射变换、MixUp、HSV增强、随机水平翻转，以及来自 albumentations 包的其他增强技术。此外，它还改进了网格敏感性，使其更稳定，不易出现梯度爆炸的情况。

YOLOv5 提供了五个不同规模的版本：YOLOv5n (微小)、YOLOv5s (小)、YOLOv5m (中) 、YOLOv5l (大) 和 YOLOv5x (特大)，其中卷积模块的宽度和深度各不相同，以适应特定的应用和硬件需求。例如，YOLOv5n 和 YOLOv5s 是针对资源有限设备的轻量级模型，而 YOLOv5x 则以牺牲速度为代价进行了高性能优化。

截至撰写本文时，YOLOv5 的发布版本是 v7.0，包括了具备分类和实例分割功能的 YOLOv5 版本。YOLOv5 是开源的，并由 Ultralytics 积极维护，拥有 250 多名贡献者，并经常进行新的改进。YOLOv5 易于使用、训练和部署。Ultralytics 还提供了适用于 iOS 和 Android 的移动版本，以及许多标注、训练和部署的集成工具。

在 MS COCO 数据集的 test-dev 2017 上评估，YOLOv5x 在 640 像素的图像尺寸下达到了 50.7% 的 AP。使用批量大小为 32，它可以在 NVIDIA V100 上实现200 FPS 的速度。使用更大的输入尺寸 (1536 像素) 和测试时间增强 (TTA)，YOLOv5 可以达到 55.8% 的 AP。

<img src="./assets/yolo_review_fig13.jpg">

**图 13**：YOLOv5 架构。该架构使用一个修改后的 CSPDarknet53 骨干网络，其包括一个 Stem，后面是从图像中提取特征的卷积层。空间金字塔池化快速 (SPPF) 层通过将特征汇聚到固定大小的特征图中来加速计算。每个卷积层都具有批量归一化和 SiLU 激活。网络的 neck 使用了 SPPF 和修改过的 CSP-PAN，而 head 似于 YOLOv3。图示基于[85]和[86]。

## 10 Scaled-YOLOv4

YOLOv4 发布一年后，同一作者团队在 2021 年的 CVPR 上提出了 Scaled-YOLOv4。与 YOLOv4 不同，Scaled YOLOv4 是基于 Pytorch 而不是 Darknet 开发的。其主要创新是引入了放大和缩小技术。放大是以牺牲速度为代价提高模型准确率；另一方面，缩小是以牺牲准确率为代价提高模型速度。此外，缩小后的模型需要更少的计算力，可以在嵌入式系统上运行。

缩小的模型架构称为 YOLOv4-tiny，它是针对低端 GPU 设计的，在 Jetson TX2 上可以以 46 FPS 运行，在 RTX2080Ti 上可以达到 440 FPS，在 MS COCO 数据集上得到 22% 的平均精度 (AP)。

放大的模型架构称为 YOLOv4-large，它包含了三种不同尺寸 P5、P6 和 P7。这个架构是为云 GPU 设计的，取得了当时最先进的性能，在 MS COCO 数据集上达到 56% 的 AP，超过之前的所有模型。

## 11 YOLOR

YOLOR 由 YOLOv4 的同一研究团队于 2021 年 5 月在 ArXiv 上发表。其名称表示“你只需要学习一个表示 (You Only Learn One Representation)”。在这篇文章中，作者采用了不同的方法；他们开发了一种多任务学习方法，旨在通过学习通用表示和使用子网络创建特定任务的表示，来创建完成各种任务 (例如分类、检测、姿态估计) 的单一模型。作者认为，传统的联合学习方法常常导致次优特征生成，YOLOR 旨在通过编码神经网络的隐式知识来克服这一点，类似于人类如何利用过去的经验来解决新问题。结果表明向神经网络中引入隐式知识有利于所有任务。

在 MS COCO 数据集 test-dev 2017 上评估，YOLOR 达到了 55.4% 的 AP 和 73.3% 的 $AP_{50}$ ，在 NVIDIA V100 上以 30 FPS 运行。

## 12 YOLOX

YOLOX 由商汤科技于 2021 年 7 月在 ArXiv 上发表。它基于 Pytorch 开发，以 Ultralytics 的 YOLOv3 为起点，做了五个主要改进：一个无锚框架构、多个正样本、解耦头部、高级标签分配和强数据增强。它在 2021 年取得了最先进的结果，在速度和准确性之间取得了最佳平衡，在 Tesla V100 上以 68.9% 的 FPS 达到 50.1% 的 AP。下面我们介绍 YOLOX 相对于 YOLOv3 的五个主要改进:

1. **无锚框**。自 YOLOv2 以来，所有后续的 YOLO 版本都是基于锚框的检测器。YOLOX 受到如 CornerNet、CenterNet 和 FCOS 等无锚框目标检测器的启发，返回到了一个简化了训练和解码过程的无锚框架构。无锚框相对于 YOLOv3 基线提高了 0.9 个点的 AP。

2. **多个正样本**。为了补偿无锚框产生的大量不平衡，作者使用中心采样，将中心 3×3 区域赋予正样本。这种方法提高了 2.1 个点的 AP。

3. **解耦头部**。在 [94,95] 中发现，分类置信度和定位准确度之间可能存在不对齐。因此，YOLOX 将它们分为两个头部 (如图 14 所示)，一个用于分类，另一个用于回归，提高了 1.1 个点的 AP，并加速了模型收敛。

4. **高级标签分配**。在 [96] 中发现，当多个对象的框重叠时，真值标签分配会存在歧义，并将分配过程建模为一个最优传输 (OT) 问题。YOLOX 受此启发，提出了一种简化的称为 simOTA 的方法。这种改进提高了 2.3 个点的 AP。

5. **强数据增强**。YOLOX 使用 MixUP 和 Mosaic 数据增强。作者发现，在使用这些增强后，ImageNet 预训练不再有益。强数据增强提高了 2.4 个点的 AP。

<img src="./assets/yolo_review_fig14.jpg">

**图 14**：YOLOv3 头部和 YOLOX 解耦头部之间的区别。对于 FPN 的每个级别，他们使用了一个 1×1 的卷积层将特征通道减少到 256，然后添加了两个平行的分支，每个分支都有两个 3×3 的卷积层，用于类别置信度 (分类) 和定位 (回归) 任务。IoU 分支添加到了回归头部。

## 13 YOLOv6

YOLOv6 由美团视觉 AI 部门于 2022 年 9 月在 ArXiv 上发表。网络设计由高效的骨干网络 (使用 RepVGG 或 CSPStackRep 模块)、PAN 结构的 neck 和混合通道策略的高效解耦 head 组成。此外，论文引入了使用后量化训练和通道方向蒸馏的增强量化技术，得到了更快更准确的检测器。总体而言，YOLOv6 在准确度和速度指标上优于之前的最新模型，如 YOLOv5、YOLOX 和 PP-YOLOE。图 15 显示了 YOLOv6 的详细架构。该模型的主要创新总结如下：

1. 一种新的基于 RepVGG 的称为 EfficientRep 的骨干网络，具有比以前的 YOLO 骨干网络更高的并行性。对于 neck，他们使用了以 RepBlocks 或 CSPStackRep 模块增强的 PAN。并且与 YOLOX 一样，他们开发了一个高效的解耦头部。

2. 使用 TOOD 中引入的任务对齐学习方法进行标签分配。

3. 新的分类和回归损失。他们使用了一个 VariFocal 分类损失和一个 SIoU /GIoU 回归损失。

4. 回归和分类任务的自蒸馏策略。 

5. 使用 RepOptimizer 和通道级别的蒸馏的检测量化方案，帮助获得更快的检测器。

作者提供了 8 个不同模型，从 YOLOv6-N 到YOLOv6-L6。在 MS COCO 数据集 test-dev 2017 上评估，最大模型在 NVIDIA Tesla T4上获得了约 29 FPS 和 57.2% 的 AP。

<img src="./assets/yolo_review_fig15.jpg">

**图 15**：YOLOv6 架构。该架构使用了一个具有 RepVGG 模块的新骨干网络。空间金字塔池化快速 (SPPF) 和卷积模块与 YOLOv5 相似。但是，YOLOv6 使用了一个解耦的头部。图示基于 [99]。

## 14 YOLOv7

YOLOv7 由 YOLOv4 和 YOLOR 的同一作者团队于 2022 年 7 月在 ArXiv 上发表。当时，它在 5 FPS 到 160 FPS 的范围内超过了所有已知的目标检测器，无论是速度还是准确率。与 YOLOv4 一样，它仅使用 MS COCO 数据集进行训练，没有使用预训练的骨干网络。YOLOv7 提出了一些架构变化和一系列 bag-of-freebies 方法，这些方法在不影响推理速度但增加了训练时间的情况下提高了准确率。

图 16 显示了 YOLOv7 的详细架构。

YOLOv7 的架构变化有：

- 扩展高效层聚合网络 (E-ELAN)。ELAN 是一种策略，允许深度模型通过控制最短和最长的梯度路径来更高效地学习和收敛。YOLOv7 提出的 E-ELAN 适用于具有无限堆叠计算块的模型。E-ELAN 通过打乱和合并基数 (cardinality) 来增强网络的学习，而不破坏原始的梯度路径。

- 基于拼接的模型缩放。缩放通过调整某些模型属性生成不同大小的模型。YOLOv7 的架构是基于拼接的架构，标准的缩放技术 (如深度缩放) 会改变过渡 (transition) 层的输入通道和输出通道的比率，从而导致模型的硬件利用率下降。YOLOv7 为基于拼接的模型缩放提出了一种新的策略，其中块的深度和宽度按相同因子缩放，以维持模型的最佳结构。

YOLOv7 使用的 bag-of-freebies 包括:

- 计划重参数化 (re-parameterized) 卷积。与 YOLOv6 类似，YOLOv7 的架构也来源于重参数化卷积 (RepConv)。但是，他们发现 RepConv 中的恒等连接破坏了 ResNet 中的残差和 DenseNet 中的拼接。因此，他们删除了恒等连接，并将其称为RepConvN。

- 为辅助头使用粗标签分配，为主头使用细标签分配。主头负责最终输出，辅助头协助训练。

- conv-bn-activation 中的 BN。这会在推理阶段将 BN 的均值和方差集成到卷积层的偏置和权重中。

- 受 YOLOR 中的隐式知识启发。

- 指数移动平均作为最终推理模型。

### 14.1 Comparison with YOLOv4 and YOLOR

在这一节中，我们重点介绍了 YOLOv7 相较于同一作者团队开发的之前 YOLO 模型的增强。

与 YOLOv4 相比，YOLOv7 在参数量减少 75%，计算量减少 36% 的同时，AP 提高了 1.5%。

与 YOLOv4-tiny 相比，YOLOv7-tiny 分别将参数量和计算量降低了 39% 和 49%，同时保持相同的 AP。

最后，与 YOLOR 相比，YOLOv7 分别将参数量和计算量降低了 43% 和 15%，同时 AP 略有提升 0.4%。

在 MS COCO 数据集 test-dev 2017 上评估，输入尺寸为 1280 像素的 YOLOv7-E6 在 NVIDIA V100 上以 50 FPS 的速度运行,取得了 55.9% 的 AP 和 73.5% 的AP50。

<img src="./assets/yolo_review_fig16.jpg">

**图 16**：YOLOv7 架构。该架构的变化包括 ELAN 模块，它通过打乱和合并基数来合并不同组的特征，增强模型学习，以及修改后的没有恒等连接的 RepVGG。图源自 [107]。

## 15 DAMO-YOLO

DAMO-YOLO 由阿里巴巴集团于 2022 年 11 月在 ArXiv 上发表。受当前技术的启发，DAMO-YOLO 包括以下几点:

1. 神经架构搜索 (NAS)。他们使用阿里巴巴开发的称为 MAE-NAS 的方法自动找到高效的架构。

2. 一个大的 neck。受 GiraffeDet、CSPNet 和 ELAN 的启发，作者设计了一个可以实时工作的称为 Efficient-RepGFPN 的 neck。

3. 一个小的 head。作者发现一个大的 neck 搭配一个小的 head 可以获得更好的性能，他们只为分类保留了一个线性层，回归也只保留了一个线性层。他们将这种方法称为 ZeroHead。

4. AlignedOTA 标签分配。动态标签分配方法如 OTA 和 TOOD 由于相较静态方法有显著改进而越来越受欢迎。然而，分类和回归之间的未对准问题仍然存在，部分原因是分类和回归损失之间的不平衡。为解决这个问题，他们的 AlignOTA 方法在分类成本中引入了 Focal Loss，并使用预测框和真值框的 IoU 作为软标签，以选择每个目标的对齐样本，从全局角度解决问题。

5. 知识蒸馏。他们提出的策略包含两个阶段：第一阶段老师指导学生，第二阶段学生独立微调。此外，他们在蒸馏方法中引入了两个增强：对齐模块适配学生特征与老师相同的分辨率，以及通道方向动态温度，将老师和学生特征标准化以减少实值差异的影响。

作者生成了一系列缩放模型 DAMO-YOLO-Tiny/Small/Medium，最佳模型在 NVIDIA V100 上以 233 FPS 达到了 50.0% 的AP。

## 16 YOLOv8

YOLOv8 由开发了 YOLOv5 的 Ultralytics 公司于 2023 年 1 月发布。YOLOv8 提供了五个缩放版本：YOLOv8n (nano)、YOLOv8s (small)、YOLOv8m (medium)、YOLOv8l (large) 和 YOLOv8x (extra large)。YOLOv8 支持多种视觉任务，如目标检测、分割、姿态估计、跟踪和分类。

### 16.1 YOLOv8 Architecture

图 17 显示了 YOLOv8 的详细架构。YOLOv8 使用与 YOLOv5 类似的骨干网络，但在 CSPLayer (现称为 C2f 模块) 上有一些改动。C2f模块 (cross-stage partial bottleneck with two convolutions) 将高级特征与上下文信息相结合，以提高检测精度。

YOLOv8 使用无锚框模型和解耦头部，独立处理是否有目标 (objectness)、分类和回归任务。这种设计使每个分支能专注于自身任务，提高了模型的整体精度。在YOLOv8 的输出层，他们使用 Sigmoid 函数作为 objectness 分数的激活函数，表示边界框包含目标的概率。对于类别概率，使用 Softmax 函数表示目标属于每个可能类别的概率。

YOLOv8 使用 CIoU 和 DFL 损失函数作为边界框损失，使用二值交叉熵作为分类损失。这些损失提高了目标检测性能，特别是在处理较小目标时。

YOLOv8 还提供了一个名为 YOLOv8-Seg 的语义分割模型。骨干网络是 CSPDarknet53 特征提取器，后接 C2f 模块代替传统的 YOLO neck 架构。C2f 模块后面是两个分割头，用于预测输入图像的语义分割掩码。该模型具有与 YOLOv8 类似的检测头。由五个检测模块和一个预测层组成。YOLOv8-Seg 模型在各种目标检测和语义分割基准测试中取得了最先进的结果，同时保持了高速和高效。

YOLOv8 可以通过命令行界面 (CLI) 运行，也可以作为 PIP 包安装。此外，它还带有多种标注、训练和部署集成。

在MS COCO 数据集 test-dev 2017 上评估，输入尺寸为 640 像素的 YOLOv8x 取得了 53.9% 的 AP (而 YOLOv5 在相同输入尺寸下为 50.7%)，在 NVIDIA A100 和TensorRT 上的速度为 280 FPS。

<img src="./assets/yolo_review_fig17.jpg">

**图 17**：YOLOv8 架构。该架构使用了经过修改的 CSPDarknet53 骨干网络。C2f 模块替代了 YOLOv5 中使用的 CSPLayer。空间金字塔池化快速 (SPPF) 层通过将特征汇聚到固定大小的特征图中来加速计算。每个卷积层都具有批量归一化和 SiLU 激活。头部被解耦，独立处理目标性、分类和回归任务。图示基于 [115]。

## 17 PP-YOLO, PP-YOLOv2, and PP-YOLOE

PP-YOLO 模型一直与我们所描述的 YOLO 模型并行发展。然而，我们决定将它们归为一组，因为它们始于 YOLOv3，并逐步改进了之前的 PP-YOLO 版本。尽管如此，这些模型在 YOLO 的演化过程中仍然很有影响力。PP-YOLO 类似于 YOLOv4 和 YOLOv5，是基于 YOLOv3 的。它于 2020 年 7 月由百度公司的研究人员在 ArXiv 上发表。作者使用了 PaddlePaddle 深度学习平台，因此得名 PP。跟随我们从 YOLOv4 开始看到的趋势，PP-YOLO 添加了十种现有技巧来提高检测器的准确性，同时保持速度不变。根据作者的说法，这篇论文的目的并不是引入新的目标检测器，而是展示如何一步步构建更好的检测器。PP-YOLO 使用的大多数技巧与 YOLOv4 使用的不同，重叠部分也使用了不同的实现。PP-YOLO 对比 YOLOv3 的变化如下:

1. 将 DarkNet-53 骨干网络替换为 ResNet50-vd，并在最后一阶段使用了增强的可变形卷积 (deformable convolutions)，和一个经过蒸馏的预训练模型，该模型在 ImageNet 上具有更高的分类准确性。该架构称为 ResNet5-vd-dcn。

2. 一个更大的批量大小以提高训练稳定性，从 64 增加到 192，并更新训练时程和学习率。

3. 维护训练参数的移动平均值，并使用它们代替最终训练的值。

4. DropBlock 仅被应用于 FPN。

5. 在另一个分支中添加 IoU 损失，与边界框回归的 L1 损失一起使用。

6. 添加 IoU 预测分支以测量位置精度，并添加 IoU 感知损失。在推理过程中，YOLOv3 将分类概率和 objectiveness 分数相乘以计算最终检测结果，PP-YOLO 还将预测的 IoU 相乘以考虑位置精度。

7. 使用类似于 YOLOv4 的网格敏感方法来改善边界框中心在网格边界处的预测。

8. 使用 Matrix NMS，它可以并行运行，比传统 NMS 更快。

9. 在 FPN 的 1x1 卷积和检测头的第一个卷积层上使用 CoordConv。CoordConv 允许网络学习平移不变性，从而提高检测的位置精度。

10. 仅在最顶层特征图上使用空间金字塔池化，以增大骨干网络的感受野。

### 17.1 PP-YOLO augmentations and preprocessing

PP-YOLO 使用以下增强和预处理：

1. 使用 Mixup 训练，权重从 $Beta(\alpha, \beta)$ 分布中采样，其中 $\alpha = 1.5, \beta =1.5$ 。

2. 随机颜色扭曲。

3. 随机扩展。

4. 随机裁剪和随机翻转，概率为0.5。

5. RGB 通道 z-score 归一化，均值为 [0.485, 0.456, 0.406]，标准差为 [0.229, 0.224, 0.225]。

6. 从 [320, 352, 384, 416, 448, 480, 512, 544, 576, 608] 中均匀抽取多个图像尺寸。

在 MS COCO 数据集 test-dev 2017 上评估，PP-YOLO 在 NVIDIA V100 上以 73 FPS 的速度取得 45.9% 的 AP 和 65.2% 的AP50。

### 17.2 PP-YOLOv2

PP-YOLOv2 于 2021 年 4 月在 ArXiv 上发表，并对 PP-YOLO 添加了四项改进，在 NVIDIA V100 上以 69 FPS 的速度将性能从 45.9% 的 AP 提高到 49.5% 的 AP。PP-YOLOv2 对比 PP-YOLO 的变化如下：

1. 骨干网络从 ResNet50 改为 ResNet101。
2. 使用路径聚合网络 (PAN) 代替 FPN，类似于 YOLOv4。
3. Mish 激活函数。与 YOLOv4 和 YOLOv5 不同，他们只在检测颈部应用了 mish 激活函数，以保持骨干网络中 ReLU 不变。
4. 更大的输入尺寸有助于提高对小目标的性能。他们将最大输入尺寸从 608 扩展到 768，并将批量大小从每 GPU 24 减小到 12 张图像。输入尺寸从[320,352,384,416,448,480,512,544,576, 608,640,672,704,736,768] 中均匀抽取。
5. 一个修改的 IoU 感知分支。他们使用软标签格式代替软权重格式修改了 IoU 感知损失的计算。

### 17.3 PP-YOLOE

PP-YOLOE 于 2022 年 3 月在 ArXiv 上发表。它在 PP-YOLOv2 的基础上进行了改进，在 NVIDIA V100 上达到了51.4% AP 和 78.1 FPS 的性能。图 18 显示了详细的架构图。PP-YOLOE 相比 PP-YOLOv2 的主要变化有：

1. 无锚框。遵循 [93, 92, 91, 90] 的工作趋势，PP-YOLOE 使用了无锚框架构。 

2. 新的骨干网络和颈部。受 TreeNet 的启发，作者使用 RepResBlocks 组合残差和密集连接修改了骨干网络和颈部的架构。

3. 任务对齐学习 (TAL)。YOLOX 首先提出了任务不对齐的问题，即分类置信度和定位精度并非在所有情况下都一致。为减少这个问题，PP-YOLOE 实现了 TOOD 中提出的TAL，其中包括动态标签分配结合任务对齐损失。

4. 高效的任务对齐的头部 (ET-head)。不同于 YOLOX 将分类和定位头解耦，PP-YOLOE 改用基于 TOOD 的单头以提高速度和准确性。

5. Varifocal (VFL) 和 Distribution Focal loss (DFL)。VFL 使用目标得分加权正样本的损失，给予 IoU 更高的样本更高的权重。这在训练中优先考虑高质量样本。类似地，两者都使用 IoU 感知分类分数 (IACS) 作为目标，允许联合学习分类和定位质量，保持训练和推理之间的一致性。另一方面，DFL 将 Focal loss 从离散扩展到连续标签，支持对混合质量估计和类别预测的改进表示的成功优化。这使得对真实数据中灵活分布的准确描绘成为可能，消除了不一致的风险。

与之前的 YOLO 版本类似，作者通过改变骨干网络和颈部的宽度和深度生成了多种缩放模型。这些模型被称为 PP-YOLOE-s (small)、PP-YOLOE-m (medium)、PP-YOLOE-l (large) 和 PP-YOLOE-x (extra large)。

<img src="./assets/yolo_review_fig18.jpg">

**图 18**：PP-YOLOE 架构。骨干网络基于 CSPRepResNet，neck 使用一个路径聚合网络，而 head 使用 ES 层以形成一个高效的任务对齐的 head (ET-head)。图示基于 [123]。

## 18 YOLO-NAS

YOLO-NAS 于 2023 年 5 月由 Deci 公司发布，Deci 是一家开发生产级模型和工具来构建、优化和部署深度学习模型的公司。YOLO-NAS 旨在检测小物体、提高定位精度和提高每计算单元的性能比，因此适用于实时边缘设备应用。另外，它的开源架构可用于研究使用。

YOLO-NAS 的创新之处包括：

- 称为 QSP 和 QCI 的量化感知模块，结合了 8 位量化的重参数化，以最小化后训练量化期间的精度损失。
- 使用 Deci 的专利 NAS 技术 AutoNAC 自动设计架构。
- 混合量化方法可选择性地量化模型的某些部分代替标准量化 (所有层都受影响)，以平衡延迟和精度。
- 使用自动标记数据、自蒸馏和大型数据集的预训练机制。

创建 YOLO-NAS 的关键 AutoNAC 系统是通用的，可以适应任何任务、数据特性、推理环境和性能目标设置。它帮助用户确定最合适的结构，为特定用途提供精度和推理速度的完美平衡。该技术考虑了数据、硬件以及推理过程中涉及的其他元素，如编译器和量化。此外，在 NAS 过程中还将 RepVGG 模块纳入模型架构，以兼容后训练量化 (PTQ)。他们通过改变 QSP 和 QCI 块的深度和位置生成了三种架构：YOLO-NASS、YOLO-NASM 和 YOLO-NASL (S、M、L分别代表小、中、大)。图 19 显示了 YOLO-NASL 的模型架构。

该模型在 Objects365 上进行了预训练，该数据集包含 200 万张图像和 365 个类别，然后使用 COCO 数据集生成了伪标签。最后，模型使用 COCO 数据集的原始118k 训练图像进行训练。

截至撰写本文时，已发布了三个 YOLO-NAS 模型，分别为 FP32、FP16 和 INT8 精度，在 16 位精度下在 MS COCO 上实现了 52.2% 的 AP。

<img src="./assets/yolo_review_fig19.jpg">

**图 19**：YOLO-NAS架构。该架构是通过名为 AutoNAC 的神经架构搜索 (NAS) 系统自动发现的，以平衡延迟和吞吐量。他们生成了三种架构，分别称为 YOLO-NASS (small)、YOLO-NASM (medium) 和 YOLO-NASL (large)，根据 QSP 和 QCI 块的深度和位置进行变化。图示展示了 YOLO-NASL 架构。

## 19 YOLO with Transformers

随着 Transformer [127] 的兴起，它接管了从语言和音频处理到视觉领域的大部分深度学习任务。因此，将 Transformer 和 YOLO 相结合是自然而然的。最早尝试将 Transformer 用于目标检测的模型之一是 You Only Look at One Sequence (YOLOS)。该模型将预训练的视觉 Transformer (ViT) 从图像分类任务迁移到目标检测任务，并在 MS COCO 数据集上取得了 42.0% 的 AP。YOLOS 主要做了两项改动：1) 将用于分类的单个 [CLS] token 替换为 100 个用于检测的 [DET] token，2) 将 ViT 中的图像分类损失替换为类似于 End-to-end object detection with transformers 的二分匹配损失。

许多研究将 Transformer 与针对特定应用的 YOLO 相关架构结合起来。例如，张等人 [131] 受到 Vision Transformer 对遮挡、扰动和域漂移的鲁棒性的启发，提出了 ViT-YOLO。该混合架构的主干网络中结合了 CSP-Darknet 和多头自注意力 (MHSA-Darknet)，双向特征金字塔网络 (BiFPN) 作为颈部网络，以及类似 YOLOv3 的多尺度检测头。他们的特定用例是无人机图像中的目标检测。图 20 显示了 ViT-YOLO 的详细架构。

MSFT-YOLO 在主干网络和检测头中加入了基于 Transformer 的模块，旨在检测钢材表面的缺陷。NRT-YOLO (Nested Residual Transformer) 则针对遥感图像中的微小目标问题。NRT-YOLO 通过添加额外的预测头、特征融合层和残差 Transformer 模块，使其在 DOTA 数据集上的性能比 YOLOv5l 提升了 5.4%。

在遥感应用中，YOLO-SD [135] 旨在提高合成孔径雷达 (SAR) 图像中小型船舶的检测精度。该模型以 YOLOX 为基础，结合多尺度卷积 (MSC) 改善不同尺度的检测，并使用特征 Transformer 模块捕捉全局特征。作者表明，这些改动使 YOLO-SD 在 HRSID 数据集上的精度优于 YOLOX。

另一个将 YOLO 与检测 Transformer (DETR) 结合的有趣尝试是 DEYO [137]。该模型由两个阶段组成：一个基于 YOLOv5 的模型，后面接一个类似 DETR 模型。第一阶段生成高质量的查询和锚点，作为第二阶段的输入。结果表明，DEYO 的收敛速度更快，性能优于 DETR，在 COCO 检测基准上实现了 52.1% 的 AP。

<img src="./assets/yolo_review_fig20.jpg">

**图 20**：ViT-YOLO架构。骨干网络 MHSA-Darknet 将多头自注意块 (MHSADark Block) 与交叉阶段部分连接块 (CSPDark block) 相结合。颈部使用 BiFPN 来聚合来自不同骨干层级的特征，头部包括五个多尺度检测头。

## 20 Discussion

本文研究了 16 个 YOLO 版本，涵盖了从原始 YOLO 模型到最新的 YOLO-NAS 的演进历程。表 4 概述了所讨论的 YOLO 版本。从中我们可以发现几个关键的模式：

- **锚框 (Anchors)**：原始的 YOLO 模型相对简单，不使用锚框，而最先进的版本则依赖于带有锚框的两阶段检测器。YOLOv2 引入了锚框，提高了边界框预测的精度。这一趋势一直持续了五年，直到 YOLOX 引入了一种无锚框的方法，并取得了最先进的成果。从那时起，后续的 YOLO 版本都放弃了使用锚框。
- **框架 (Framework)**：最初，YOLO 使用 Darknet 框架开发，后续版本也沿袭了这一框架。然而，当 Ultralytics 将 YOLOv3 移植到 PyTorch 后，后续的 YOLO 版本都开始使用 PyTorch 开发，这带来了性能的显著提升。另一个被利用的深度学习框架是 PaddlePaddle，这是一个由百度最初开发的开源框架。
- **主干网络 (Backbone)**：YOLO 模型的主干网络架构随着时间的推移发生了重大变化。从最初的 Darknet 架构 (由简单的卷积和最大池化层组成) 开始，后来的模型逐渐引入了 YOLOv4 中的跨阶段部分连接 (CSP)、YOLOv6 和 YOLOv7 中的重新参数化，以及 DAMO-YOLO 和 YOLO-NAS 中的神经架构搜索。
- **性能 (Performance)**：虽然 YOLO 模型的性能随着时间的推移有所提高，但值得注意的是，它们通常更注重速度和精度的平衡，而不是单纯追求精度。这种权衡对 YOLO 框架至关重要，它允许在各种应用中进行实时物体检测。

<img src="./assets/yolo_review_table4.jpg">

**表 4**：YOLO架构摘要。YOLO 和 YOLOv2 报告的指标是基于 VOC2007，而其余的报告是基于 COCO2017。NAS-YOLO 模型的报告精度为 16 位。

### 20.1 Tradeoff between speed and accuracy

YOLO 系列物体检测模型始终致力于在速度和精度之间取得平衡，目标是在不牺牲检测结果质量的情况下提供实时性能。随着 YOLO 框架的不断迭代，这种权衡一直贯穿始终，每个版本都以不同的方式优化这两个相互竞争的目标。原始的 YOLO 模型主要关注实现高速物体检测。该模型使用单个卷积神经网络 (CNN) 直接从输入图像预测物体的位置和类别，从而实现实时处理。然而，这种对速度的强调不可避免地导致了精度的下降，尤其是在处理小型物体或具有重叠边界框的物体时。

后续的 YOLO 版本引入了改进和增强功能来解决这些限制，同时保持框架的实时处理能力。例如，YOLOv2 (YOLO9000) 引入了锚框和直通层来改善物体的定位，从而提高了精度。此外，YOLOv3 通过采用多尺度特征提取架构来增强模型的性能，从而可以在各种尺度上更好地检测物体。

随着 YOLO 框架的演进，速度和精度之间的权衡变得更加微妙。YOLOv4 和 YOLOv5 等模型引入了创新技术，例如新的网络主干结构、改进的数据增强技术和优化的训练策略。这些发展带来了显著的精度提升，同时没有显著影响模型的实时性能。

从 YOLOv5 开始，所有官方的 YOLO 模型都微调了速度和精度之间的平衡，提供不同的模型规模以适应特定的应用和硬件需求。例如，这些版本通常提供针对边缘设备优化的轻量级模型，以降低计算复杂度和加快处理速度来换取精度。图 21 展示了 YOLOv5 到 YOLOv8 的不同模型规模的比较。该图提供了 YOLO 模型不同版本在复杂性和性能方面的对比分析。左图将参数数量 (百万) 与 COCO 验证集上的涵盖了 50 到 95 的 IOU 阈值的平均精度 (mAP) 进行对比。它清晰地表明，参数数量的增加会提高模型的精度。每个模型都包含多个尺度，分别用 n (微小)、s (小)、m (中)、l (大) 和 x (超大) 表示。

右图对比了使用 NVIDIA A100 GPU、TensorRT FP16 进行推理时的推理延迟和相同的 mAP 性能指标。这里可以明显看出推理速度和检测精度之间的权衡。较低的延迟值表示更快的模型推理，通常会导致较低的精度。相反，具有更高延迟的模型往往在 COCO mAP 指标上取得更好的性能。这种关系对于实时处理至关重要的应用程序来说至关重要，模型的选择会受到平衡速度和精度需求的影响。

<img src="./assets/yolo_review_fig21.jpg">

**图 21**：YOLO 物体检测模型的性能比较。左侧子图展示了模型复杂度 (参数数量) 与检测精度 (COCO mAP50-95) 之间的关系。右侧子图则展示了推理速度 (A100 TensorRT FP16 上的延迟) 和相同模型的精度之间的权衡。每个模型版本都用不同的颜色表示，标记则代表从 nano 到 extra 的不同变体。该图引用自 [138]。

## 21 The future of YOLO

随着 YOLO 框架的不断发展，我们预计以下的趋势和可能性将会塑造其未来的走向：

- **引入最新技术**：研究人员和开发者将继续通过利用深度学习、数据增强和训练技术的最先进方法来改进 YOLO 架构。这种持续的创新可能会提高模型的性能、鲁棒性和效率。
- **性能基准的演进**：目前用于评估物体检测模型的基准 COCO 2017 最终可能会被更先进和更具挑战性的基准所取代。这类似于前两个 YOLO 版本使用的 VOC 2007 基准的转变，反映了随着模型变得更加复杂和准确，需要更苛刻的基准测试。
- **YOLO 模型和应用的激增**：随着 YOLO 框架的进步，我们预计每年发布的 YOLO 模型数量将增加，相应的应用领域也将扩大。随着框架变得更加通用和强大，它可能会被用于更多不同的领域，从家用电器设备到自动驾驶汽车。
- **扩展到新领域**：YOLO 模型除了物体检测和分割之外，还可能扩展到其他领域，例如视频中的物体跟踪和 3D 关键点估计。我们预计 YOLO 模型将过渡到多模态框架，融合视觉和语言、视频以及声音处理。随着这些模型的发展，它们可能会成为满足更广泛计算机视觉和多媒体任务的创新解决方案的基础。
- **适应性更强的硬件支持**：YOLO 模型将进一步跨越硬件平台，从物联网设备到高性能计算集群。这种适应性将使 YOLO 模型能够根据应用程序的需求和限制在各种场景下部署。此外，通过使模型适应不同的硬件规格，YOLO 可以使其对更多用户和行业更易使用且更有效。

