# [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)

## Abstract

最近，自然语言处理在大规模数据上的模型预训练取得了重大突破，为计算机视觉中类似的基础模型铺平了道路。这些模型可以通过生成通用的视觉特征，即在不需要微调的情况下适用于各种图像分布和任务的特征，极大地简化了任何系统中的图像使用。这项工作表明，现有的预训练方法，尤其是自监督方法，如果在足够多样化的数据源上进行训练，可以产生这种通用的特征。我们重新审视了现有的方法，并结合不同的技术在数据量和模型大小方面扩展我们的预训练。在技术贡献方面，我们主要关注在大规模训练中加速和稳定训练的方法。在数据方面，我们提出了一个自动流水线，用于构建专门、多样化且经过筛选的图像数据集，而不是通常在自监督文献中使用的未筛选数据。在模型方面，我们训练了一个拥有 10 亿 (1B) 参数的 ViT 模型 (Dosovitskiy 等人，2021年)，并将其蒸馏成一系列较小的模型，这些模型在大多数图像和像素级别的基准测试中超越了现有的通用特征 OpenCLIP (Ilharco等人，2021年)。

## 1 Introduction

在自然语言处理 (NLP) 领域，学习与任务无关的预训练表示已经成为标准做法 (Radford 等人，2019年；Raffel 等人，2020年；Chowdhery 等人，2022年；Hoffmann等人，2022年；Touvron等人，2023年 )。人们可以直接使用这些特征，即不需要微调，就能在下游任务中取得明显优于任务特定的模型 (Brown等人，2020年 ) 的性能。这一成功得益于在大量原始文本上进行预训练，使用了不需要标注的预训练目标，例如语言建模 (Radford等人，2017年 ) 或词向量 (Devlin等人，2019年 ) 。

在 NLP 领域的这一范式转变之后，我们预计类似的“基础”模型将出现在计算机视觉领域 (Bommasani等人，2021年 )。这些模型应该生成能够直接适用于任何任务的视觉特征，无论是在图像级别 (例如图像分类 ) 还是像素级别 (例如分割 ) 上。对于这些基础模型，最有前途的工作集中在文本引导的预训练上，即使用一种文本监督形式来指导特征的训练 (Joulin等人，2016年；Mahajan等人，2018年；Radford等人，2021年 ) 。这种文本引导的预训练限制了关于图像的信息的保留，因为描述 (captions) 只是近似反映图像中丰富的信息，而复杂的像素级信息可能无法通过这种监督方法表现出来。此外，这些图像编码器需要对齐的文本-图像语料库，因此不具备其文本对应物的灵活性，即仅从原始数据 (row data) 中学习。

文本指导预训练的另一种替代方法是自监督学习 (Caron等人，2018年；Chen等人，2020年；He等人，2022年)，它从图像中学习特征。这些方法在概念上更接近于诸如语言建模的预训练任务，并且可以捕捉图像和像素级别的信息 (Caron等人，2021年) 。此外，自监督模型输出的特征已被证明具有各种有用的属性，并且已经实现了多种应用 (Amir等人，2022年；Tumanyan等人，2022年；Ofri-Amar等人，2023年；Hamilton等人，2022年)。然而，尽管自监督学习有潜力学习通用特征，但大部分的进展都是在小规模策划数据集 ImageNet-1k (Russakovsky等人，2015年) 上进行的。一些尝试将这些方法扩展到 ImageNet-1k 之外的数据集 (Caron等人，2019年；Goyal等人，2021年；2022a年) 都集中在未筛选的数据集上，这通常会导致特征质量的显著下降。这是因为缺乏对数据质量和多样性的控制，而这些因素对于生成良好特征至关重要。

在这项工作中，我们探讨了如果自监督学习在大规模定制数据上进行预训练，是否有潜力学习通用的视觉特征。我们重新审视了现有的判别性自监督方法，这些方法在图像和补丁级别上学习特征，例如 iBOT (Zhou等人，2022a)，并在更大数据集的视角下重新考虑了其中一些设计选择。我们的大部分技术贡献都针对在模型和数据规模扩展时稳定和加速判别性自监督学习。这些改进使我们的方法比类似的判别性自监督方法快约 2 倍，并且内存需求减少了 3 倍，从而使我们能够利用更大批次大小进行更长时间的训练。

关于预训练数据，我们构建了一个自动化流水线，从大量未筛选的图像集合中筛选和重新平衡数据集。这个流水线受到了自然语言处理 (NLP) 中使用的流水线的启发 (Wenzek等人，2020年) ，其使用数据相似性而不是外部元数据，并且不需要手动注释。在处理自然图像时，一个主要难点是重新平衡概念并避免过度拟合于少数主导模式。在这项工作中，一个朴素的聚类方法就能比较好地解决这个问题。我们收集了一个小但多样化的，包含 142M 张图像的语料库来验证我们的方法。

最后，我们提供了各种预训练的视觉模型，称为 DINOv2，它们是在我们的数据上使用不同的 Vision Transformers (ViT)  (Dosovitskiy等人，2016年) 架构进行训练的。我们发布了所有模型和可以在任何数据上重新训练 DINOv2 的代码。我们通过在图像和像素级别上对 DINOv2 在各种计算机视觉基准上的性能进行评估，总结如图 2 所示。我们得出结论，自监督预训练本身是一个很好的候选，可以学习与最佳公开可用的弱监督模型具有竞争性的可迁移的固定特征。



## 10 Future work and Discussion

在这项工作中，我们介绍了 DINOv2，这是一系列在大型筛选数据上进行无监督预训练的图像编码器。这是首个在图像数据上进行自监督学习的工作，其视觉特征在广泛的基准测试中与 (弱) 监督替代方案的性能差距不大，并且无需微调。我们可以将 DINOv2 系列模型的强大性能归因于以下几个因素：i) 改进的训练配方，包括更好的超参数和正则化 (表 1)，ii) 更大的模型规模，无论用于训练的数据如何，性能都有改进 (图 4)，iii) 更大的数据集 (图 4)，以及iv) 蒸馏过程使较小的模型受益于最强的 ViT-g 模型的性能 (图 5) 。这些模型呈现出一些特性，例如对物体部分和场景几何的理解，无论图像领域如何。我们预计在更大规模的模型和数据下，会出现更多这些特性，类似于大型语言模型中的指令出现，并计划继续在这些方面进行扩展。此外，本文还证明了这些视觉特征与简单的分类器 (例如线性层) 兼容，这意味着底层信息是可以直接获取的。在未来的工作中，我们计划利用这种能力来训练一个具有语言功能的 AI 系统，该系统可以处理视觉特征，就像它们是词 token 一样，并提取所需的信息来支持系统。