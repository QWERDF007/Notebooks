# [Generative Adversarial Nets](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)

## Abstract

我们提出一种通过对抗性过程来估计生成模型的新框架，我们同时训练两个模型：一个捕获数据分布的生成模型 $G$，和一个估计样本是来自训练数据而不是 $G$ 的概率的判别模型 $D$。$G$ 的训练过程是最大化 $D$ 犯错的概率。这个框架对应于一个极大极小的双人博弈。在任意函数 $G$ 和 $D$ 的空间中，存在一个唯一解，使得 $G$ 可以恢复训练数据的分布，且 $D$ 处处等于 $\frac{1}{2}$。在 $G$ 和 $D$ 是由多层感知机定义的情况下，整个系统可以使用反向传播进行训练。在整个训练和样本生成过程中，不需要任何的马尔可夫链或者展开近似的推理网络。实验通过对生成的样本的定性和定量评估，证明了该框架的潜力。

## 1 Introduction

深度学习的承诺 (promise) 是发现丰富的、有层次的模型 [2]，这些模型表示人工智能应用中遇到的各种数据的概率分布，比如自然图像、包含语音的音频波形和自然语言语料库中的符号。到目前为止，深度学习中最惊人的成功已经涉及判别模型，这些模型通常将一个高维的、富有感官的输入映射到一个类别标签。这些惊人的成功主要是基于反向传播和 dropout 算法，使用具有特别好梯度的分段线性单元。由于在最大似然估计和相关策略中的许多难以近似的棘手的概率计算，以及在生成环境中难以利用分段线性单元的好处，深度生成模型的影响较小。我们提出一种新的生成模型估计程序来避开这些困难。

在提出的对抗网络框架中，生成模型是与一个对手对抗的：一个判别模型，它学习确定一个样本是来自模型分布还是数据分布。生成模型可以被认为是一个造假的团队，试图产生假的假币并使用它而不被发现，而判别模型类似警察，试图发现假币。这场比赛的竞争促使两支队伍改进它们的方法，直到仿冒品与真品难以区分。

**ps：** 最终希望生成模型能胜过判别模型，然后能生成与真实分布一致的数据。

该框架可以为多种模型和优化算法提供特定的训练算法。在本文中，我们探讨了当生成模型通过将一个随机噪声传递给一个多层感知机来生成样本，并且判别模型同样是一个多层感知机的特殊情况。我们把这种特殊情况称为对抗网络。在这种情况下，我们可以只使用非常成功的反向传播和 dropout 算法训练两个模型，并只使用前向传播从生成模型中采用。不需要近似推理或者马尔可夫链。

## 2 Related work

直到最近，大多数关于深度生成模型都集中在为概率分布函数提供参数规范的模型上。然后可以通过最大化对数似然来训练模型。在这一系列模型中，也许最成功的是深度玻尔兹曼机。这类模型通常具有难以处理的似然函数，因此需要对似然梯度进行多次近似。这些困难推动了 "生成机器"——模型不能显式地表示似然，但能从期望的分布中生成样本。生成随机网络是生成机器的一个例子，它可以用精确的反向传播来训练，而不是玻尔兹曼机器所需的无数次近似。这项工作通过消除生成随机网络中使用的马尔可夫链，扩展了生成机的思想。

我们的工作通过使用以下的观察将梯度反向传播给生成过程：
$$
\large \lim_{\sigma \rightarrow 0} \nabla_x \mathbb{E}_{\epsilon \sim \mathcal{N}(0,\sigma^2 I) f(x + \epsilon)}
$$
在我们开展这项工作时，我们不知道 Kingma 和 Welling [18] 和 Rezende 等人 [23] 已经开展了更通用的随机反向传播规则，允许模型通过具有有限方差的高斯分布来反向传播，并反向传播到协方差参数和均值。这些反向传播规则可以让模型学习生成器的条件方差，我们在本工作中把它当作超参数。Kingma 和 Welling 和 Rezende 等人使用随机反向传播训练变分自编码器 (VAEs)。像生成对抗网络一样，变分自编码器将一个可微分生成器网络与第二个神经网络配对。不像生成对抗网络，VAE 中的第二个神经网络是一个执行近似推理的识别模型。GANs 需要通过可视单元进行微分，因此不能对离散数据进行建模，而 VAEs 需要通过隐藏单元进行微分，因此不能有离散的潜在变量。存在着其他的 [12, 22] 类似 VAE 的方法，但与我们的方法的关系不太密切。

先前的工作 [29,13] 也采用了使用一个辨别损失函数来训练一个生成模型的方法。
