# [Generative Adversarial Nets](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)

## Abstract

我们提出一种通过对抗性过程来估计生成式模型的新框架，我们同时训练两个模型：一个捕获数据分布的生成式模型 $G$，和一个估计样本是来自训练数据而不是 $G$ 的概率的判别式模型 $D$。$G$ 的训练过程是最大化 $D$ 犯错的概率。这个框架对应于一个极大极小的双人博弈。在任意函数 $G$ 和 $D$ 的空间中，存在一个唯一解，使得 $G$ 可以恢复训练数据的分布，且 $D$ 处处等于 $\frac{1}{2}$。在 $G$ 和 $D$ 是由多层感知机定义的情况下，整个系统可以使用反向传播进行训练。在整个训练和样本生成过程中，不需要任何的马尔可夫链或者展开近似的推理网络。实验通过对生成的样本的定性和定量评估，证明了该框架的潜力。

## 1 Introduction

深度学习的承诺 (promise) 是发现丰富的、有层次的模型 [2]，这些模型表示人工智能应用中遇到的各种数据的概率分布，比如自然图像、包含语音的音频波形和自然语言语料库中的符号。到目前为止，深度学习中最惊人的成功已经涉及到判别式模型，这些模型通常将一个高维的、富有感官的输入映射到一个类别标签。这些惊人的成功主要是基于反向传播和 dropout 算法，使用具有特别好梯度的分段线性单元。由于在最大似然估计和相关策略中的许多难以近似的棘手的概率计算，以及在生成环境中难以利用分段线性单元的好处，深度生成式模型的影响较小。我们提出一种新的生成式模型估计程序来避开这些困难。

在提出的对抗网络框架中，生成式模型是与一个对手对抗的：一个判别式模型，它学习判定一个样本是来自模型分布还是数据分布。生成式模型可以被认为是一个造假的团队，试图产生假的假币并使用它而不被发现，而判别式模型类似警察，试图发现假币。这场比赛的竞争促使两支队伍改进它们的方法，直到仿冒品与真品难以区分。

**ps：** 最终希望生成式模型能胜过判别式模型，然后能生成与真实分布一致的数据。

这个框架可以为多种模型和优化算法提供特定的训练算法。在本文中，我们探讨了当生成式模型通过将一个随机噪声传递给一个多层感知机来生成样本，并且判别式模型同样是一个多层感知机的特殊情况。我们把这种特殊情况称为对抗网络。在这种情况下，我们可以只使用非常成功的反向传播和 dropout 算法训练两个模型，并只使用前向传播从生成式模型中采样。不需要近似推理或者马尔可夫链。

## 2 Related work

直到最近，大多数关于深度生成式模型都集中在为概率分布函数提供参数规范的模型上。然后可以通过最大化对数似然来训练模型。在这一系列模型中，也许最成功的是深度玻尔兹曼机。这类模型通常具有难以处理的似然函数，因此需要对似然梯度进行无数次近似。这些困难推动了 "生成式机器"——模型不显式地表示似然，但能从期望的分布中生成样本。生成随机网络是生成式机器的一个例子，它可以用精确的反向传播来训练，而不是玻尔兹曼机器所需的无数次近似。这项工作通过消除生成随机网络中使用的马尔可夫链，扩展了生成式机器的思想。

我们的工作通过使用以下的观察将梯度反向传播给生成过程：
$$
\large \lim_{\sigma \rightarrow 0} \nabla_x \mathbb{E}_{\epsilon \sim \mathcal{N}(0,\sigma^2 I) f(x + \epsilon)}
$$
在我们开展这项工作时，我们不知道 Kingma 和 Welling [18] 和 Rezende 等人 [23] 已经研究出了更通用的随机反向传播规则，允许模型通过具有有限方差的高斯分布来反向传播，并反向传播到协方差参数和均值。这些反向传播规则可以让模型学习生成器的条件方差，我们在本工作中把它当作超参数。Kingma 和 Welling 和 Rezende 等人使用随机反向传播训练变分自编码器 (VAEs)。像生成式对抗网络一样，变分自编码器将一个可微分生成器网络与第二个神经网络配对。不像生成式对抗网络，VAE 中的第二个神经网络是一个执行近似推理的识别模型。GANs 需要通过可视单元进行微分，因此不能对离散数据进行建模，而 VAEs 需要通过隐藏单元进行微分，因此不能有离散的潜在变量。存在着其他的 [12, 22] 类似 VAE 的方法，但与我们的方法的关系不太密切。

先前的工作 [29,13] 也采用了使用一个判别式准测来训练一个生成式模型的方法。这些方法使用的准则是深度生成式模型难以处理的。深度模型甚至很难近似这些方法，因为它们涉及到概率的比率，不能用降低概率边界的变分近似来近似。噪声对比估计 (NCE) 涉及通过学习使模型在从固定噪声分布中区分数据有用的权重来训练生成式模型。使用先前训练过的模型作为噪声分布，可以训练一系列质量不断提高的模型。这可以被视为一种非正式的竞争机制，在精神上类似于对抗性网络游戏中使用的正式的竞争。NCE 的关键限制是它的 "鉴别器" 是由噪声分布和模型分布的概率密度的比率来定义的，因此需要评估和反向传播两个密度的能力。

先前的一些工作使用了一般意义上的两个神经网络对抗的概念。最相关的工作是可预测性最小化。在可预测性最小化中，神经网络中的每个隐藏单元都被训练成不同于第二个网络的输出，第二个网络是在给定其他隐藏层神经元的值的情况下预测那个隐藏层神经元的值。本项工作在三个重要方面不同于可预测性最小化：1) 在这项工作中，网络之间的竞争是唯一的训练准则，它本身足以支撑训练网络。可预测性最小化仅是一个正则器，它促进网络中的隐藏单元在完成其他任务时保持统计上的独立性；这不是首要的训练准则。2) 竞争的本质不同。在可预测性最小化中，两个网络的输出进行比较，其中一个网络试图使两个网络的输出相似，而另外一个网络试图使两个网络的输出不同。所讨论的输出是一个标量。在 GANs 中，一个网络产生一个多样的、高维的向量作为另外一个网络的输入，并试图选择一个使另外一个网络不知道如何处理的输入。3) 学习过程的规范不同。可预测性最小化被描述为一个最小化目标函数的优化问题，并学习逼近目标函数的最小值。GANs 是基于极大极小博弈而不是优化问题的，它有一个价值函数，一个 agent 寻求最大化，而另外一个寻求最小化。博弈终止于一个鞍点，这个鞍点对于一方的策略是最小值，对于另一方的策略是最大值。











REFERENCE:

https://rdc.hundsun.com/portal/article/920.html
