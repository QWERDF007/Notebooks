# Attention Is All You Need

## Abstract

主要的序列转换模型是基于复杂的循环/或卷积神经网络，包含一个编码器和一个解码器。表现最好的模型也通过一个注意力机制连接着编码器和编码器。我们提出一个新的简单的网络架构，Transformer，仅仅基于注意力机制，完全不需要循环和卷积。在两个机器翻译任务上的实验表明这些模型在质量上优于其他模型，同时具有更好的并行性，并且需要的时间明显更少。我们的模型在 WMT 2014 English-to-German 翻译任务上取得了 28.4 [BLEU]()，比现有的最佳模型，包括集成，提高了 2 BLEU 以上。在 WMT 2014 English-to-French 翻译任务上，我们的模型在 8 个 GPUs 上训练了 3.5 天后，建立了一个新的单模型 start-of-the-art BLEU，分数为 41.8，这是现有文献中最好的模型的训练成本的一小部分。通过将 Transformer 成功地应用到大型的和有限的训练数据的 English constituency parsing (英语成分句法分析)，我们证明了 Transformer 是可以很好地泛化到其他任务。

## 1 Introduction

循环神经网络，特别是长短期记忆 (LSTM) 和门控循环神经网络，已经被牢牢地确定为序列建模和转导问题 (如语言建模和机器翻译) 的 state-of-the-art 方法。此后，许多工作继续推动循环语言模型的边界和编码器-解码器的架构。

循环模型通常沿着输入和输出序列的符号位置分解计算。在计算时间内将 positions 与 steps 对齐，它们生成一系列隐藏状态 $h_t$，作为前一个隐藏状态 $h_{t-1}$ 的函数和位置 $t$ 的输入。这种内在的顺序性妨碍了训练样本内的并行化，并行化在序列长度较长时很重要，因为内存的约束限制了跨样本的批处理。最近的工作通过因子分解技巧 [21] 和条件计算 [32] 在计算效率方面取得了显著的提升，同时后者也提升了模型性能。然而，顺序计算的基本约束仍然存在。

注意力机制已经成为各种任务中的引人注目的序列建模和转导模型的不可或缺的一部分，它允许对依赖关系建模，而不考虑它们在输入或输出序列的距离。然而，在所有的情况下，除了 [27]，这种注意力机制是与循环神经网络一起使用的。

在本文中，我们提出了 Transformer，一种避开循环且完全依赖注意力机制来绘制输入和输出之间的全局依赖的的模型架构。Transformer 允许有更多的并行化，并在 8 个 P100 GPUs 上训练 12 个小时后，在翻译质量上达到了一个新的 state-of-the-art。

## 2 Background









## 7 Conclusion

本工作中，我们提出了 Transformer，第一个完全基于注意力的序列转导模型，用多头自注意力替换了编码器-解码器架构中最常用的循环层。

对于翻译任务，Transformer 能比基于循环层或卷积层的架构显著地训练得更快。在 WMT 2014 English-to-German 和 WMT 2014 English-to-French 翻译任务中，我们取得了一个新的 state-of-the-art。在前一个任务中，我们最好的模型胜过所有之前报告的集成 (ensembles)。

我们对基于注意力的模型的未来感到兴奋，并计划将其应用到其他任务。我们计划将 Transformer 扩展到涉及文本以外的输入和输出形态的问题，并研究局部的、受限的注意力机制，以有效地处理大量的输入和输出，如图像、音频和视频。生成少量的序列是我们的另外一个研究目标。
