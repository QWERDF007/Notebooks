# Attention Is All You Need

## Abstract

主要的序列转录模型是基于复杂的循环/或卷积神经网络，包含一个编码器和一个解码器。最好的模型也通过一个注意力机制连接着编码器和编码器。我们提出一个新的简单的网络架构，Transformer，仅仅基于注意力机制，完全不需要循环和卷积。在两个机器翻译任务上的实验表明这些模型在质量上优于其他模型，同时具有更好的并行性，并且需要的时间明显更少。我们的模型在 WMT 2014 English-to-German 翻译任务上取得了 28.4 [BLEU]()，比现有的最佳模型，包括集成，提高了 2 BLEU 以上。在 WMT 2014 English-to-French 翻译任务上，我们的模型在 8 个 GPUs 上训练了 3.5 天后，建立了一个新的单模型 start-of-the-art BLEU，分数为 41.8，这是现有文献中最好的模型的训练成本的一小部分。通过将 Transformer 成功地应用到大型的和有限的训练数据的 English constituency parsing (英语成分句法分析)，我们证明了 Transformer 是可以很好地泛化到其他任务。

## 1 Introduction

循环神经网络，特别是长短期记忆 (LSTM) 和门控循环神经网络，已经被牢牢地确定为序列建模和转录问题 (如语言建模和机器翻译) 的 state-of-the-art 方法。此后，许多工作继续推动循环语言模型的边界和编码器-解码器的架构。

循环模型通常沿着输入和输出序列的符号位置分解计算。在计算时间内将 positions 与 steps 对齐，它们生成一系列隐藏状态 $h_t$，作为前一个隐藏状态 $h_{t-1}$ 和位置 $t$ 的输入的函数。这种内在的时许性妨碍了训练样本内的并行化，并行化在序列长度较长时很重要，因为内存的约束限制了跨样本的批处理。最近的工作通过因子分解技巧 [21] 和条件计算 [32] 在计算效率方面取得了显著的提升，同时后者也提升了模型性能。然而，顺序计算的基本约束仍然存在。

注意力机制已经成为各种任务中的引人注目的序列建模和转录模型的不可或缺的一部分，它允许对依赖关系建模，而不考虑它们在输入或输出序列的距离。然而，在所有的情况下，除了 [27]，这种注意力机制是与循环神经网络一起使用的。

在本文中，我们提出了 Transformer，一种避开循环神经层而完全依赖注意力机制来绘制输入和输出之间的全局依赖的的模型架构。Transformer 允许有明显更多的并行化，并在 8 个 P100 GPUs 上训练 12 个小时后，在翻译质量上达到了一个新的 state-of-the-art。

## 2 Background

减少时许的计算的目标也构成了 Extended Neural GPU、ByteNet 和 ConvS2S 的基础，所有这些都使用卷积神经网络作为基础构建块，并行计算所有输入和输出位置的隐藏表示。在这些模型中，将任意两个来自输入或者输出位置的信号关联起来所需的操作数随着位置之间的距离而增长，对于 ConS2S 是线性增长，对于 ByteNet 是对数级增长。这使得学习相隔遥远的位置之间的依赖变得更加困难。在 Transformer 中，这被减少到一个常量级的操作数，虽然是以降低有效分辨率为代价的，但由于对注意力加权的位置的平均，我们在 3.2 节中使用多头注意力抵消了这种影响。

自注意力，有时也被叫做内部注意力，是一种将单个序列的不同位置关联起来的一种注意力机制，以便计算该序列的表示。自注意力已经成功地被应用于多种任务，包括阅读理解、摘要总结、文本蕴含和学习任务无关的句子表示。

端到端的记忆网络是基于一个循环注意力机制而不是序列对齐的循环，并已被证明在简单语言问题问答和语言建模任务种表现得好。

然而，据我们所知，Transformer 是第一个不使用序列对齐的 RNNs 或卷积，完全依赖自注意力来计算其输入和输出的表示的语言转录模型。在下面的章节，我们将描述 Transformer，自注意力的动机和讨论它相对于 [17, 18] 和 [9] 这些模型的优势。

## 3 Model Architecture

大多数有竞争性的神经序列转录模型都有一个编码器-解码器结构。解码器映射一个符号表示的输入序列 $(x_1,\cdots,x_n)$ 到一个连续表示的序列 $\bold{z} = (z_1, \cdots, z_n)$。给定 $\bold{z}$，解码器生成一个符号的输出序列 $(y_1,\cdots,y_m)$，每次一个元素。在每个步骤中，模型都是自回归的，在生成下一个符号时使用先前生成的符号作为额外的输入。

Transformer 遵循这种整体架构，编码器和解码器都使用堆叠的自注意力和逐点的、完全连接的层，分布展示在图 1 的左半和右半部分。

<img src="assets/Transformer_fig1.png" title="图1">

**图 1：** Transformer-模型架构

### 3.1 Encoder and Decoder Stacks

**编码器：** 编码器由一个 N = 6 个相同的层的堆叠组成。每层有两个子层。第一层是一个多头自注意力机制，而第二层是一个简单的，逐位置完全连接的前馈网络。我们在两个子层周围分别使用一个残差连接，然后使用层归一化。即每个子层的输出是 $\rm LayerNorm(x + Sublayer(x))$，其中 $\rm Sublayer(x)$ 是由子层自身实现的函数。为了方便这些残差连接，这个模型中的所有子层以及嵌入层都产生维度 $d_{model} = 512$ 的输出。

**解码器：** 解码器也是由一个 N = 6 个相同的层的堆叠组成。除了每个解码器中的两个子层，解码器插入第三个子层，它在编码器堆叠的输出上执行多头注意力。与编码器类似，我们在每个子层周围采用残差连接，然后使用层归一化。我们也修改解码器堆叠中的自注意力子层，以防止位置参与后续的位置。这种遮蔽，再加上输出嵌入被偏移了一个位置，确保了位置 $i$ 的预测仅能够依赖位置小于 $i$ 的已知输出。

### 3.2 Attention

注意力函数可以被描述为将一个查询 (query) 和一组键值 (key-value) 对映射到一个输出。其中查询、键、值和输出都是向量。输出是计算值的加权和，其中分配给每个值的权重是由查询与相对应的键通过一个 compatibility 函数计算。

<img src="assets/Transformer_fig2.png" title="图2">

**图 2：** (左) 缩放的点积注意力。(右) 由几个并行运行注意力层组成的多头注意力。

#### 3.2.1 Scaled Dot-Product Attention

我们将我们特别关注的注意力称为 "缩放的点积注意力"。输入由 $d_k$ 维的查询和键，和 $d_v$ 维的值组成。我们计算查询与所有的键的点积，将结果除以 $\sqrt{d_k}$ ，并应用 softmax 函数来获得值的权重。

实践中，我们同时在一组查询上计算注意力函数，将它们打包成一个矩阵 $Q$。键和值也打包成矩阵 $K$ 和 $V$。我们计算输出矩阵如下：
$$
\rm Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V \tag{1}
$$
最常用的两个注意力函数是加性注意力和点积 (乘法的) 注意力。点积注意力与我们的算法相同，除了缩放因子 $\frac{1}{\sqrt{d_k}}$。加性注意力使用一个具有单一隐藏层的前馈网络计算 compatibility 函数。虽然这两种方法的理论复杂度相似，但实际上点积注意力更快且空间效率更高，因为它能用高度优化的矩阵乘法代码来实现。

当 $d_k$ 的值较小时，这两种机制的表现相似，但当 $d_k$ 较大时，加性注意力优于没有缩放的点积注意力。我们怀疑当 $d_k$ 较大时，点积会变得很大，将 softmax 函数推到了梯度极小的区域。为了抵消这种影响，我们用 $\frac{1}{\sqrt{d_k}}$ 缩放点积。

#### 3.2.2 Multi-Head Attention



## 7 Conclusion

本工作中，我们提出了 Transformer，第一个完全基于注意力的序列转录模型，用多头自注意力替换了编码器-解码器架构中最常用的循环层。

对于翻译任务，Transformer 能比基于循环层或卷积层的架构显著地训练得更快。在 WMT 2014 English-to-German 和 WMT 2014 English-to-French 翻译任务中，我们取得了一个新的 state-of-the-art。在前一个任务中，我们最好的模型胜过所有之前报告的集成 (ensembles)。

我们对基于注意力的模型的未来感到兴奋，并计划将其应用到其他任务。我们计划将 Transformer 扩展到涉及文本以外的输入和输出形态的问题，并研究局部的、受限的注意力机制，以有效地处理大量的输入和输出，如图像、音频和视频。使生成的不那么时序化是我们的另一研究目标。
