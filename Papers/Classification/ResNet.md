# Deep Residual Learning for Image Recognition

## Abstract

更深的神经网络更难训练。我们提出一个残差学习框架，使得训练比以前的网络深得多的网络容易。我们明确地将层重新定义为学习关于输入的残差，而不是学习无参照的函数。我们提供了全面的实验性证据表明这些残差网络更加容易优化，并且可以从大幅增加的深度中提升精度。在 ImageNet 数据集上，我们评估深度高达 152 层的残差网络——比 VGG 网络深 8 倍，但仍然具有更低的复杂性。一个残差网络的集成 (ensemble) 在 ImageNet 测试集上取得了 3.57% 的错误率。这个结果在 ILSVRC 2015 分类任务中获得了第一名。我们还用 100 层和 1000 层在 CIFAR-10 上进行了分析。

表征的深度对许多视觉识别任务至关重要。仅仅由于我们的极其深的表征，我们在 COCO 目标检测数据集上获得了 28% 的相对提升。深度残差网络是我们在 ILSVRC & COCO 2015 竞赛中的提交的基线，在那我们还赢得了 ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割的第一名。

## 1. Introduction

深度卷积神经网络为图像分类带来了一系列的突破。深度网络以端到端的多层方式自然地集成了低/中/高级特征和分类器，而特征的 "级别" 可以通过堆叠层的数量 (深度) 来丰富。最近的证据表明网络的深度是至关重要的，而在具有挑战性的 ImageNet 数据集位于前列的结果都利用了 "非常深" 的模型，深度从 16 到 30。许多其他重要的视觉识别任务也从非常深的模型中获益良多。

受深度的重要性的驱使，一个问题出现了：学习更好的网络就像堆叠更多的层一样容易吗？回答这个问题的一个障碍是臭名昭著的梯度消失/爆炸问题，它从一开始就阻碍了收敛。然而，这个问题很大程度上通过归一化后的初始化和中间的归一化层得到了解决，它使得具有数十层的网络开始收敛于使用反向传播的随机梯度下降 (SGD)。

当更深的网络开始收敛时，一个退化问题就暴露出来了：随着网络的深度的增加，准确性达到饱和 (可能并不奇怪)  然后迅速退化。令人意外的是，这种退化并不是过拟合造成的，在适当深度的模型上添加更多的层会导致更高的训练误差，如 [11, 42] 中所报道的，并通过我们的实验得到验证。图 1 展示了一个典型的例子。

<img src="assets/ResNet_fig1.png" title="图1">

图 1：使用 20 层和 50 层 "普通" 网络在 CIFAR-10 上的训练错误率 (左) 和 测试错误率 (右)。更深的网络有更高的训练错误率和测试错误率。ImageNet 上同样的现象在图 4 中展示。

(训练精度的) 退化表明并非所有的系统都同样容易优化。让我们考虑一个较浅的架构以及在它基础上添加更多层的较深的对应架构。通过构造更深的模型，存在一种解决方案：添加的层是恒等映射，而其他的层是从学习后的较浅的模型拷贝而来。这种构造方法的存在表明，深度模型应该不会产生比浅层模型更高的训练误差。但实验表明，我们现有的 solvers 无法找到比这种构建方案更好的解决方案 (或无法在可行时间内做到)。

在本论文中，我们通过引入一种深度残差学习框架来解决退化问题。与其希望每几个堆叠的层直接拟合一个期望的底层映射，不如显示地让这些层拟合一个残差映射。形式上，将期望的底层映射表示为 $\mathcal{H}(\bold{x})$，我们让堆叠的非线性层拟合另外一个映射 $\mathcal{F}(\bold{x}) := \mathcal{H}(\bold{x}) - \bold{x}$。原来的映射被改写成 $\mathcal{F}(\bold{x}) + \bold{x}$。我们假设优化残差映射比优化原始的更容易。最极端的情况是，如果一个恒等映射是最优的，那么将残差推向 0 比通过一堆非线性层来拟合恒等映射更容易。

表达式 $\mathcal{F}(\bold{x}) + \bold{x}$ 可以使用 "shortcut connections" (图 2) 的前馈神经网络来实现。Shortcut connections 是指跳过一层或多层的连接。在我们的方法中，shortcut connections 简单地执行恒等映射，它们的输出被添加到堆叠的层的输出上 (图 2)。恒等 shortcut connections 既不增加参数，也不增加计算复杂性。整个网络仍然可以通过 SGD 使用反向传播进行端到端的训练，并且可以很容易地使用常用的库 (如 Caffe) 实现，无需修改 solvers。

<img src="assets/ResNet_fig2.png" title="图2">

图 2：残差学习：一个构建块

我们在 ImageNet 上进行了全面的实验，以展示退化问题并评估我们的方法。我们的研究表明：1) 我们的非常深的残差网络容易优化，但对应的 "普通" 网络 (简单地堆叠层) 在深度增加时表现出更高的训练误差；2) 我们的深度残差网络可以很容易地从大幅度增加的深度中获得精度提升，产生比之前的网络好很多的结果。

在 CIFAR-10 数据集上也显示了类似的现象，这表明我们的方法的优化的困难和效果并不仅仅是与特定的数据集有关系。我们在这个数据集上成功训练了 100 层的模型，并探索了 1000 层以上的模型。

在 ImageNet 分类数据集上，我们通过非常深的残差网络获得了极好的结果。我们的 152 层的残差网络是 ImageNet 上出现过的最深的网络，但复杂度比 VGG 网络更低。我们的集成 (ensemble) 在 ImageNet 测试集上获得了 3.57% 的 top-5 错误率，并赢得了 ILSVRC 2015 分类竞赛的第一名。非常深的表征在其他识别任务上也有极好的泛化性能，并让我们在 ILSVRC & COCO 2015 竞赛中的 ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割取得了第一名。这一强有力的证据表明，残差学习的原则是通用的，我们希望它能适用于其他视觉和非视觉的问题。

## 2. Related Work

**Residual Representations.** 在图像识别中，[VLAD](https://hal.inria.fr/inria-00633013/document) 是一个与字典相关的残差向量进行编码的表示，而 [Fisher Vector](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.7388&rep=rep1&type=pdf) 可以表示为 VLAD 的概率版本。两者都是图像检索和分类的强有力的浅层表示。对于矢量量化，编码残差向量被证明比编码原始向量更加有效。

在低级的视觉和计算机图形学中，为了求解偏微分方程 (PDEs)，广泛使用的多重网格法将系统重新表示为多尺度的子问题，其中每个子问题负责粗尺度和细尺度之间的残差解。多重网格的另外一种选择是 hierarchical basis preconditioning，它依赖于表示两个尺度之间的残差向量的变量。已有研究表明，这些 solvers 比那些不知道残差本质的标准 solvers 收敛快得多。这些方法表明，好的 reformulation 和 preconditioning 能简化优化。

**Shortcut Connections.** Shortcut connections 的实践和理论已经被研究了很长时间。一个训练多层感知机 (MLPs) 的早期实践是添加一个线性层连接网络的输入到输出。在 [44, 24]，一些中间层直接连接到辅助分类器，用于处理梯度消失/爆炸。论文 [39, 38, 31, 47] 提出通过 shortcut connections 实现集中层响应、梯度和传播误差。在 [44]，一个 "inception" 层由一个 shortcut 分支和几个更深的分支组成。

与我们的工作的同一时期，"highway networks" 提出了有门函数的 shortcut connections。这些门是依赖于数据并且有参数，相反，我们的恒等 shortcuts 则没有参数。当门控 shortcut "关闭" (接近 0)，highway networks 中的层代表非残差函数。相反，我们的 formulation 则总是学习残差函数；我们的恒等 shortcuts 永远不会关闭，所有的信息总是被通过，还有一些额外的要学习的残差函数。另外，highway networks 没有展示随着深度的极大提升 (例如超过 100 层) 获得精度的提升。

## 3. Deep Residual Learning

### 3.1. Residual Learning

让我们把 $\mathcal{H}(\bold{x})$ 看作是由几个堆叠的层 (不一定是整个网络) 拟合的底层映射，$\bold{x}$ 表示这些层的第一层的输入。如果假设多个非线性层能渐进地逼近复杂的函数，那么就等价于假设它们可以渐进地逼近残差函数，即 $\mathcal{H}(\bold{x}) - \bold{x}$ (假设输入和输出是相同维度的)。因此，与其期望堆叠的层逼近 $\mathcal{H}(\bold{x})$，不如显示地让这些层逼近一个残差函数 $\mathcal{F}(\bold{x}) := \mathcal{H}(\bold{x}) - \bold{x}$。因此，原始函数就变成了 $\mathcal{F}(\bold{x}) + \bold{x}$。虽然这两种形式都应该能够渐进地逼近期望函数 (如假设的那样)，但学习的难易程度可能不同。

这个 reformulation 是由关于退化问题的反直觉现象 (图 1 左) 所激励的。正如我们在引言中所讨论的，如果添加的层可以被构建为恒等映射，一个更深的模型的训练误差应该不大于它的对应的浅层模型。退化问题表明 solvers 在用多个非线性层逼近恒等映射时可能有困难。使用残差学习 reformulation，如果恒等映射是最优的，solvers 可以简单地将多个非线性层的权重趋于零来逼近恒等映射。

在实际情况中，恒等映射不太可能是最优的，但我们的 reformulation 可能有助于 precondition 问题。如果最优的函数是接近于恒等映射而不是零映射，那么对于 solvers 来说，找到一个关于恒等映射的扰动比学习一个新函数更容易。我们通过实验 (图 7) 表明学习到的残差函数整体上响应较小，这表明恒等映射提供了合理的 preconditioning。

### 3.2. Identity Mapping by Shortcuts

