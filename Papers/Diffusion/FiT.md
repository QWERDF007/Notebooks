# [FiT: Flexible Vision Transformer for Diffusion Model](https://arxiv.org/abs/2402.12376)

## Abstract

自然界不存在分辨率的限制。在这个现实背景下，现有的扩散模型，比如 Diffusion Transformers，经常在处理训练域之外的图像分辨率时面临挑战。为了克服这一局限性，我们提出了 Flexible Vision Transformer (FiT)，这是一种专门为生成不受限制分辨率和宽高比的图像而设计的 transformer 架构。与传统的将图像视为静态分辨率网格的方法不同，FiT 将图像概念化为动态大小的 tokens 的序列。这种视角启发了一种灵活的训练策略，在训练和推理阶段都可以毫不费力地适应不同的宽高比，从而促进分辨率泛化和消除图像裁剪导致的偏差。在精心调整的网络结构和无需训练的外推技术的增强下，FiT 在分辨率外推生成方面表现出卓越的灵活性。全面实验展示了 FiT 在广泛分辨率范围内的卓越性能，展示了其在训练分辨率分布内外的有效性。代码库链接：https://github.com/whlzy/FiT。

## 1. Introduction

当前的图像生成模型在跨任意分辨率上的泛化能力存在困难。尽管 Diffusion Transformer (DiT) (Peebles和Xie, 2023) 系列在某些分辨率范围内表现出色，但在处理不同分辨率的图像时仍存在不足。这一局限性源于 DiT 在训练过程中无法利用动态分辨率图像，从而影响了其对不同 token 长度或分辨率的有效适应能力。

为了克服这一局限，我们提出了Flexible Vision Transformer (FiT)，它善于生成任意分辨率和宽高比的图像。其关键动机是对图像数据建模的新视角：FiT 不再将图像视为固定维度的静态网格，而是将其概念化为可变长度的 token 序列。这种方法使 FiT 能够动态调整序列长度，从而在不受预定义维度限制的情况下生成任意所需分辨率的图像。通过高效管理可变长度的 token 序列并将其填充到最大指定长度，FiT 解锁了与分辨率无关的图像生成潜力。FiT 在灵活的训练流程、网络架构和推理过程方面取得了显著的进展，体现了这一范式转变。

**灵活的训练流程**。FiT 在训练过程中独特地保留了原始图像的宽高比，将图像视为 token 序列。这种独特的视角使 FiT 能够自适应地调整高分辨率图像的大小，以适应预定义的最大 token 限制，确保无论原始分辨率如何，图像都不会被裁剪或不成比例地缩放。这种方法保持了图像分辨率的完整性，如图 2 所示，有助于在各种分辨率下生成高保真度的图像。据我们所知，FiT 是第一个在整个训练过程中保持多样化图像分辨率的基于 transformer 的生成模型。

**网络架构**。FiT 模型是从 DiT 架构演变而来，但解决了其在分辨率外推方面的局限性。处理不同图像大小的一个关键网络架构调整是采用了 2D 旋转位置嵌入 (RoPE) (Su等,2024)，受到其在增强大型语言模型 (LLMs) 进行长度外推 (Liu等，2023年) 方面的成功启发。我们还引入了 Swish-Gated Linear Unit (SwiGLU) (Shazeer，2020年)，以取代传统的多层感知器 (MLP)，并将 DiT 的多头自注意力 (MHSA) 替换为 Masked MHSA，以在我们的灵活训练流程中高效管理填充 token。

**推理过程**。虽然大型语言模型采用 token 长度外推技术 (Peng等, 2023; LocalLLaMA) 来生成任意长度的文本，但将这些技术直接应用于 FiT 会产生次优结果。我们针对 2D RoPE 定制这些技术，从而提升了 FiT 在各种分辨率和宽高比下的性能。

我们最高 Gflop 的 FiT-XL/2 模型，仅在 ImageNet-256 (Deng等,2009) 数据集上训练 180 万 steps，就在 $160 \times 320, 128 \times 384 , 320 \times 320 , 224 \times 448$ 和 $160 \times 480$ 等分辨率上明显超过所有最先进的 CNN 和 transformer 模型。FiT-XL/2 的性能进一步通过我们的无需训练的分辨率外推方法得到提升。与基线 DiT-XL/2 训练 700万 steps 相比，FiT-XL/2 在 $256 \times 256$ 分辨率上略微落后，但在所有其他分辨率上都明显优于它。

总之，我们的贡献在于提出了 FiT，这是一种专为扩散模型定制的灵活视觉 transformer，能够生成任意分辨率和长宽比的图像。我们在 FiT 中提出了三项创新设计：灵活的训练流程消除了裁剪需求；一种用于动态 token 长度建模的独特 transformer 架构；以及一种无需训练的分辨率外推方法来进行任意分辨率生成。严格的实验表明，FiT-XL/2 模型在各种分辨率和宽高比设置下都达到了最先进的性能。

## 5. Conclusion

在这项工作中，我们旨在为正在进行的关于灵活生成任意分辨率和宽高比的研究作出贡献。我们为扩散模型提出了 Flexible Vision Transformer (FiT)，这是一种经过优化的 transformer 架构，具有专门为生成任意分辨率和宽高比图像而设计的灵活训练流程。FiT 在各种分辨率上优于之前所有的基于 transformer 或 CNN 的模型。随着我们的分辨率外推方法 VisionNTK，FiT 的性能得到了进一步显著改进。
